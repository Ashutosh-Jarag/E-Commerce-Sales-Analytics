{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a5221db4-aa16-43e0-909f-dd8de5f3814e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello from Databricks!\nSpark version: 4.0.0\n"
     ]
    }
   ],
   "source": [
    "print(\"Hello from Databricks!\")\n",
    "print(f\"Spark version: {spark.version}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7dc975cd-7cb2-4afd-abe8-b31b7ae938d5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>id</th><th>fruit</th></tr></thead><tbody><tr><td>1</td><td>Apple</td></tr><tr><td>2</td><td>Banana</td></tr><tr><td>3</td><td>Orange</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         1,
         "Apple"
        ],
        [
         2,
         "Banana"
        ],
        [
         3,
         "Orange"
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "id",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "fruit",
         "type": "\"string\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_data = [(1, \"Apple\"), (2, \"Banana\"), (3, \"Orange\")]\n",
    "test_df = spark.createDataFrame(test_data, [\"id\", \"fruit\"])\n",
    "display(test_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "051f5f23-b29f-4d0d-8eb4-5c814a684a4d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>product_category_name</th><th>product_category_name_english</th></tr></thead><tbody><tr><td>beleza_saude</td><td>health_beauty</td></tr><tr><td>informatica_acessorios</td><td>computers_accessories</td></tr><tr><td>automotivo</td><td>auto</td></tr><tr><td>cama_mesa_banho</td><td>bed_bath_table</td></tr><tr><td>moveis_decoracao</td><td>furniture_decor</td></tr><tr><td>esporte_lazer</td><td>sports_leisure</td></tr><tr><td>perfumaria</td><td>perfumery</td></tr><tr><td>utilidades_domesticas</td><td>housewares</td></tr><tr><td>telefonia</td><td>telephony</td></tr><tr><td>relogios_presentes</td><td>watches_gifts</td></tr><tr><td>alimentos_bebidas</td><td>food_drink</td></tr><tr><td>bebes</td><td>baby</td></tr><tr><td>papelaria</td><td>stationery</td></tr><tr><td>tablets_impressao_imagem</td><td>tablets_printing_image</td></tr><tr><td>brinquedos</td><td>toys</td></tr><tr><td>telefonia_fixa</td><td>fixed_telephony</td></tr><tr><td>ferramentas_jardim</td><td>garden_tools</td></tr><tr><td>fashion_bolsas_e_acessorios</td><td>fashion_bags_accessories</td></tr><tr><td>eletroportateis</td><td>small_appliances</td></tr><tr><td>consoles_games</td><td>consoles_games</td></tr><tr><td>audio</td><td>audio</td></tr><tr><td>fashion_calcados</td><td>fashion_shoes</td></tr><tr><td>cool_stuff</td><td>cool_stuff</td></tr><tr><td>malas_acessorios</td><td>luggage_accessories</td></tr><tr><td>climatizacao</td><td>air_conditioning</td></tr><tr><td>construcao_ferramentas_construcao</td><td>construction_tools_construction</td></tr><tr><td>moveis_cozinha_area_de_servico_jantar_e_jardim</td><td>kitchen_dining_laundry_garden_furniture</td></tr><tr><td>construcao_ferramentas_jardim</td><td>costruction_tools_garden</td></tr><tr><td>fashion_roupa_masculina</td><td>fashion_male_clothing</td></tr><tr><td>pet_shop</td><td>pet_shop</td></tr><tr><td>moveis_escritorio</td><td>office_furniture</td></tr><tr><td>market_place</td><td>market_place</td></tr><tr><td>eletronicos</td><td>electronics</td></tr><tr><td>eletrodomesticos</td><td>home_appliances</td></tr><tr><td>artigos_de_festas</td><td>party_supplies</td></tr><tr><td>casa_conforto</td><td>home_confort</td></tr><tr><td>construcao_ferramentas_ferramentas</td><td>costruction_tools_tools</td></tr><tr><td>agro_industria_e_comercio</td><td>agro_industry_and_commerce</td></tr><tr><td>moveis_colchao_e_estofado</td><td>furniture_mattress_and_upholstery</td></tr><tr><td>livros_tecnicos</td><td>books_technical</td></tr><tr><td>casa_construcao</td><td>home_construction</td></tr><tr><td>instrumentos_musicais</td><td>musical_instruments</td></tr><tr><td>moveis_sala</td><td>furniture_living_room</td></tr><tr><td>construcao_ferramentas_iluminacao</td><td>construction_tools_lights</td></tr><tr><td>industria_comercio_e_negocios</td><td>industry_commerce_and_business</td></tr><tr><td>alimentos</td><td>food</td></tr><tr><td>artes</td><td>art</td></tr><tr><td>moveis_quarto</td><td>furniture_bedroom</td></tr><tr><td>livros_interesse_geral</td><td>books_general_interest</td></tr><tr><td>construcao_ferramentas_seguranca</td><td>construction_tools_safety</td></tr><tr><td>fashion_underwear_e_moda_praia</td><td>fashion_underwear_beach</td></tr><tr><td>fashion_esporte</td><td>fashion_sport</td></tr><tr><td>sinalizacao_e_seguranca</td><td>signaling_and_security</td></tr><tr><td>pcs</td><td>computers</td></tr><tr><td>artigos_de_natal</td><td>christmas_supplies</td></tr><tr><td>fashion_roupa_feminina</td><td>fashio_female_clothing</td></tr><tr><td>eletrodomesticos_2</td><td>home_appliances_2</td></tr><tr><td>livros_importados</td><td>books_imported</td></tr><tr><td>bebidas</td><td>drinks</td></tr><tr><td>cine_foto</td><td>cine_photo</td></tr><tr><td>la_cuisine</td><td>la_cuisine</td></tr><tr><td>musica</td><td>music</td></tr><tr><td>casa_conforto_2</td><td>home_comfort_2</td></tr><tr><td>portateis_casa_forno_e_cafe</td><td>small_appliances_home_oven_and_coffee</td></tr><tr><td>cds_dvds_musicais</td><td>cds_dvds_musicals</td></tr><tr><td>dvds_blu_ray</td><td>dvds_blu_ray</td></tr><tr><td>flores</td><td>flowers</td></tr><tr><td>artes_e_artesanato</td><td>arts_and_craftmanship</td></tr><tr><td>fraldas_higiene</td><td>diapers_and_hygiene</td></tr><tr><td>fashion_roupa_infanto_juvenil</td><td>fashion_childrens_clothes</td></tr><tr><td>seguros_e_servicos</td><td>security_and_services</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "beleza_saude",
         "health_beauty"
        ],
        [
         "informatica_acessorios",
         "computers_accessories"
        ],
        [
         "automotivo",
         "auto"
        ],
        [
         "cama_mesa_banho",
         "bed_bath_table"
        ],
        [
         "moveis_decoracao",
         "furniture_decor"
        ],
        [
         "esporte_lazer",
         "sports_leisure"
        ],
        [
         "perfumaria",
         "perfumery"
        ],
        [
         "utilidades_domesticas",
         "housewares"
        ],
        [
         "telefonia",
         "telephony"
        ],
        [
         "relogios_presentes",
         "watches_gifts"
        ],
        [
         "alimentos_bebidas",
         "food_drink"
        ],
        [
         "bebes",
         "baby"
        ],
        [
         "papelaria",
         "stationery"
        ],
        [
         "tablets_impressao_imagem",
         "tablets_printing_image"
        ],
        [
         "brinquedos",
         "toys"
        ],
        [
         "telefonia_fixa",
         "fixed_telephony"
        ],
        [
         "ferramentas_jardim",
         "garden_tools"
        ],
        [
         "fashion_bolsas_e_acessorios",
         "fashion_bags_accessories"
        ],
        [
         "eletroportateis",
         "small_appliances"
        ],
        [
         "consoles_games",
         "consoles_games"
        ],
        [
         "audio",
         "audio"
        ],
        [
         "fashion_calcados",
         "fashion_shoes"
        ],
        [
         "cool_stuff",
         "cool_stuff"
        ],
        [
         "malas_acessorios",
         "luggage_accessories"
        ],
        [
         "climatizacao",
         "air_conditioning"
        ],
        [
         "construcao_ferramentas_construcao",
         "construction_tools_construction"
        ],
        [
         "moveis_cozinha_area_de_servico_jantar_e_jardim",
         "kitchen_dining_laundry_garden_furniture"
        ],
        [
         "construcao_ferramentas_jardim",
         "costruction_tools_garden"
        ],
        [
         "fashion_roupa_masculina",
         "fashion_male_clothing"
        ],
        [
         "pet_shop",
         "pet_shop"
        ],
        [
         "moveis_escritorio",
         "office_furniture"
        ],
        [
         "market_place",
         "market_place"
        ],
        [
         "eletronicos",
         "electronics"
        ],
        [
         "eletrodomesticos",
         "home_appliances"
        ],
        [
         "artigos_de_festas",
         "party_supplies"
        ],
        [
         "casa_conforto",
         "home_confort"
        ],
        [
         "construcao_ferramentas_ferramentas",
         "costruction_tools_tools"
        ],
        [
         "agro_industria_e_comercio",
         "agro_industry_and_commerce"
        ],
        [
         "moveis_colchao_e_estofado",
         "furniture_mattress_and_upholstery"
        ],
        [
         "livros_tecnicos",
         "books_technical"
        ],
        [
         "casa_construcao",
         "home_construction"
        ],
        [
         "instrumentos_musicais",
         "musical_instruments"
        ],
        [
         "moveis_sala",
         "furniture_living_room"
        ],
        [
         "construcao_ferramentas_iluminacao",
         "construction_tools_lights"
        ],
        [
         "industria_comercio_e_negocios",
         "industry_commerce_and_business"
        ],
        [
         "alimentos",
         "food"
        ],
        [
         "artes",
         "art"
        ],
        [
         "moveis_quarto",
         "furniture_bedroom"
        ],
        [
         "livros_interesse_geral",
         "books_general_interest"
        ],
        [
         "construcao_ferramentas_seguranca",
         "construction_tools_safety"
        ],
        [
         "fashion_underwear_e_moda_praia",
         "fashion_underwear_beach"
        ],
        [
         "fashion_esporte",
         "fashion_sport"
        ],
        [
         "sinalizacao_e_seguranca",
         "signaling_and_security"
        ],
        [
         "pcs",
         "computers"
        ],
        [
         "artigos_de_natal",
         "christmas_supplies"
        ],
        [
         "fashion_roupa_feminina",
         "fashio_female_clothing"
        ],
        [
         "eletrodomesticos_2",
         "home_appliances_2"
        ],
        [
         "livros_importados",
         "books_imported"
        ],
        [
         "bebidas",
         "drinks"
        ],
        [
         "cine_foto",
         "cine_photo"
        ],
        [
         "la_cuisine",
         "la_cuisine"
        ],
        [
         "musica",
         "music"
        ],
        [
         "casa_conforto_2",
         "home_comfort_2"
        ],
        [
         "portateis_casa_forno_e_cafe",
         "small_appliances_home_oven_and_coffee"
        ],
        [
         "cds_dvds_musicais",
         "cds_dvds_musicals"
        ],
        [
         "dvds_blu_ray",
         "dvds_blu_ray"
        ],
        [
         "flores",
         "flowers"
        ],
        [
         "artes_e_artesanato",
         "arts_and_craftmanship"
        ],
        [
         "fraldas_higiene",
         "diapers_and_hygiene"
        ],
        [
         "fashion_roupa_infanto_juvenil",
         "fashion_childrens_clothes"
        ],
        [
         "seguros_e_servicos",
         "security_and_services"
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "product_category_name",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "product_category_name_english",
         "type": "\"string\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df = spark.read.csv(\"/Volumes/sales_analysis/data/data/product_category_name_translation.csv\", \n",
    "                    header=True, \n",
    "                    inferSchema=True)\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6d3f6f27-c290-479d-9e37-4cfd5089b057",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>path</th><th>name</th><th>size</th><th>modificationTime</th></tr></thead><tbody><tr><td>dbfs:/Volumes/sales_analysis/data/data/olist_customers_dataset.csv</td><td>olist_customers_dataset.csv</td><td>9033957</td><td>1765039803000</td></tr><tr><td>dbfs:/Volumes/sales_analysis/data/data/olist_geolocation_dataset.csv</td><td>olist_geolocation_dataset.csv</td><td>61273883</td><td>1765039869000</td></tr><tr><td>dbfs:/Volumes/sales_analysis/data/data/olist_order_items_dataset.csv</td><td>olist_order_items_dataset.csv</td><td>15438671</td><td>1765039823000</td></tr><tr><td>dbfs:/Volumes/sales_analysis/data/data/olist_order_payments_dataset.csv</td><td>olist_order_payments_dataset.csv</td><td>5777138</td><td>1765039786000</td></tr><tr><td>dbfs:/Volumes/sales_analysis/data/data/olist_order_reviews_dataset.csv</td><td>olist_order_reviews_dataset.csv</td><td>14451670</td><td>1765039821000</td></tr><tr><td>dbfs:/Volumes/sales_analysis/data/data/olist_orders_dataset.csv</td><td>olist_orders_dataset.csv</td><td>17654914</td><td>1765039827000</td></tr><tr><td>dbfs:/Volumes/sales_analysis/data/data/olist_products_dataset.csv</td><td>olist_products_dataset.csv</td><td>2379446</td><td>1765039773000</td></tr><tr><td>dbfs:/Volumes/sales_analysis/data/data/olist_sellers_dataset.csv</td><td>olist_sellers_dataset.csv</td><td>174703</td><td>1765039757000</td></tr><tr><td>dbfs:/Volumes/sales_analysis/data/data/product_category_name_translation.csv</td><td>product_category_name_translation.csv</td><td>2613</td><td>1765039755000</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "dbfs:/Volumes/sales_analysis/data/data/olist_customers_dataset.csv",
         "olist_customers_dataset.csv",
         9033957,
         1765039803000
        ],
        [
         "dbfs:/Volumes/sales_analysis/data/data/olist_geolocation_dataset.csv",
         "olist_geolocation_dataset.csv",
         61273883,
         1765039869000
        ],
        [
         "dbfs:/Volumes/sales_analysis/data/data/olist_order_items_dataset.csv",
         "olist_order_items_dataset.csv",
         15438671,
         1765039823000
        ],
        [
         "dbfs:/Volumes/sales_analysis/data/data/olist_order_payments_dataset.csv",
         "olist_order_payments_dataset.csv",
         5777138,
         1765039786000
        ],
        [
         "dbfs:/Volumes/sales_analysis/data/data/olist_order_reviews_dataset.csv",
         "olist_order_reviews_dataset.csv",
         14451670,
         1765039821000
        ],
        [
         "dbfs:/Volumes/sales_analysis/data/data/olist_orders_dataset.csv",
         "olist_orders_dataset.csv",
         17654914,
         1765039827000
        ],
        [
         "dbfs:/Volumes/sales_analysis/data/data/olist_products_dataset.csv",
         "olist_products_dataset.csv",
         2379446,
         1765039773000
        ],
        [
         "dbfs:/Volumes/sales_analysis/data/data/olist_sellers_dataset.csv",
         "olist_sellers_dataset.csv",
         174703,
         1765039757000
        ],
        [
         "dbfs:/Volumes/sales_analysis/data/data/product_category_name_translation.csv",
         "product_category_name_translation.csv",
         2613,
         1765039755000
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "path",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "name",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "size",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "modificationTime",
         "type": "\"long\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# List all files in your volume\n",
    "display(dbutils.fs.ls(\"/Volumes/sales_analysis/data/data/\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a6a7e577-c56d-426c-b052-325a2c4b89cc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "base_path = \"/Volumes/sales_analysis/data/data/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e9c60e93-568f-4970-a31e-70012e1ddd09",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 1. Customers Dataset\n",
    "df_customers_dataset = spark.read.csv(\n",
    "    base_path + \"olist_customers_dataset.csv\",\n",
    "    header=True,\n",
    "    inferSchema=True\n",
    ")\n",
    "\n",
    "# 2. Geolocation Dataset\n",
    "df_geolocation = spark.read.csv(\n",
    "    base_path + \"olist_geolocation_dataset.csv\",\n",
    "    header=True,\n",
    "    inferSchema=True\n",
    ")\n",
    "\n",
    "# 3. Order Items Dataset\n",
    "df_order_items = spark.read.csv(\n",
    "    base_path + \"olist_order_items_dataset.csv\",\n",
    "    header=True,\n",
    "    inferSchema=True\n",
    ")\n",
    "\n",
    "# 4. Order Payments Dataset\n",
    "df_order_payments = spark.read.csv(\n",
    "    base_path + \"olist_order_payments_dataset.csv\",\n",
    "    header=True,\n",
    "    inferSchema=True\n",
    ")\n",
    "\n",
    "# 5. Order Reviews Dataset\n",
    "df_order_reviews = spark.read.csv(\n",
    "    base_path + \"olist_order_reviews_dataset.csv\",\n",
    "    header=True,\n",
    "    inferSchema=True\n",
    ")\n",
    "\n",
    "# 6. Orders Dataset\n",
    "df_orders = spark.read.csv(\n",
    "    base_path + \"olist_orders_dataset.csv\",\n",
    "    header=True,\n",
    "    inferSchema=True\n",
    ")\n",
    "\n",
    "# 7. Products Dataset\n",
    "df_products = spark.read.csv(\n",
    "    base_path + \"olist_products_dataset.csv\",\n",
    "    header=True,\n",
    "    inferSchema=True\n",
    ")\n",
    "\n",
    "# 8. Sellers Dataset\n",
    "df_sellers = spark.read.csv(\n",
    "    base_path + \"olist_sellers_dataset.csv\",\n",
    "    header=True,\n",
    "    inferSchema=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "74129eb3-e728-4c52-81d7-58da3df82753",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ All datasets loaded successfully!\n\n\uD83D\uDCCC Dataset Row Counts:\n  • Customers: 99,441 rows\n  • Geolocation: 1,000,163 rows\n  • Orders: 99,441 rows\n  • Order Items: 112,650 rows\n  • Order Payments: 103,886 rows\n  • Order Reviews: 104,162 rows\n  • Products: 32,951 rows\n  • Sellers: 3,095 rows\n"
     ]
    }
   ],
   "source": [
    "print(\"✅ All datasets loaded successfully!\\n\")\n",
    "print(\"\uD83D\uDCCC Dataset Row Counts:\")\n",
    "\n",
    "print(f\"  • Customers: {df_customers_dataset.count():,} rows\")\n",
    "print(f\"  • Geolocation: {df_geolocation.count():,} rows\")\n",
    "print(f\"  • Orders: {df_orders.count():,} rows\")\n",
    "print(f\"  • Order Items: {df_order_items.count():,} rows\")\n",
    "print(f\"  • Order Payments: {df_order_payments.count():,} rows\")\n",
    "print(f\"  • Order Reviews: {df_order_reviews.count():,} rows\")\n",
    "print(f\"  • Products: {df_products.count():,} rows\")\n",
    "print(f\"  • Sellers: {df_sellers.count():,} rows\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "01d1fb61-b41b-4cbb-b8bb-6f565c0929ce",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n\uD83D\uDD0D DATA EXPLORATION - SAMPLE ROWS FROM EACH TABLE\n============================================================\n\n\uD83D\uDC65 === CUSTOMERS (Sample) ===\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>customer_id</th><th>customer_unique_id</th><th>customer_zip_code_prefix</th><th>customer_city</th><th>customer_state</th></tr></thead><tbody><tr><td>06b8999e2fba1a1fbc88172c00ba8bc7</td><td>861eff4711a542e4b93843c6dd7febb0</td><td>14409</td><td>franca</td><td>SP</td></tr><tr><td>18955e83d337fd6b2def6b18a428ac77</td><td>290c77bc529b7ac935b93aa66c333dc3</td><td>9790</td><td>sao bernardo do campo</td><td>SP</td></tr><tr><td>4e7b3e00288586ebd08712fdd0374a03</td><td>060e732b5b29e8181a18229c7b0b2b5e</td><td>1151</td><td>sao paulo</td><td>SP</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "06b8999e2fba1a1fbc88172c00ba8bc7",
         "861eff4711a542e4b93843c6dd7febb0",
         14409,
         "franca",
         "SP"
        ],
        [
         "18955e83d337fd6b2def6b18a428ac77",
         "290c77bc529b7ac935b93aa66c333dc3",
         9790,
         "sao bernardo do campo",
         "SP"
        ],
        [
         "4e7b3e00288586ebd08712fdd0374a03",
         "060e732b5b29e8181a18229c7b0b2b5e",
         1151,
         "sao paulo",
         "SP"
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "customer_id",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "customer_unique_id",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "customer_zip_code_prefix",
         "type": "\"integer\""
        },
        {
         "metadata": "{}",
         "name": "customer_city",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "customer_state",
         "type": "\"string\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n\uD83D\uDED2 === ORDERS (Sample) ===\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>order_id</th><th>customer_id</th><th>order_status</th><th>order_purchase_timestamp</th><th>order_approved_at</th><th>order_delivered_carrier_date</th><th>order_delivered_customer_date</th><th>order_estimated_delivery_date</th></tr></thead><tbody><tr><td>e481f51cbdc54678b7cc49136f2d6af7</td><td>9ef432eb6251297304e76186b10a928d</td><td>delivered</td><td>2017-10-02T10:56:33.000Z</td><td>2017-10-02T11:07:15.000Z</td><td>2017-10-04T19:55:00.000Z</td><td>2017-10-10T21:25:13.000Z</td><td>2017-10-18T00:00:00.000Z</td></tr><tr><td>53cdb2fc8bc7dce0b6741e2150273451</td><td>b0830fb4747a6c6d20dea0b8c802d7ef</td><td>delivered</td><td>2018-07-24T20:41:37.000Z</td><td>2018-07-26T03:24:27.000Z</td><td>2018-07-26T14:31:00.000Z</td><td>2018-08-07T15:27:45.000Z</td><td>2018-08-13T00:00:00.000Z</td></tr><tr><td>47770eb9100c2d0c44946d9cf07ec65d</td><td>41ce2a54c0b03bf3443c3d931a367089</td><td>delivered</td><td>2018-08-08T08:38:49.000Z</td><td>2018-08-08T08:55:23.000Z</td><td>2018-08-08T13:50:00.000Z</td><td>2018-08-17T18:06:29.000Z</td><td>2018-09-04T00:00:00.000Z</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "e481f51cbdc54678b7cc49136f2d6af7",
         "9ef432eb6251297304e76186b10a928d",
         "delivered",
         "2017-10-02T10:56:33.000Z",
         "2017-10-02T11:07:15.000Z",
         "2017-10-04T19:55:00.000Z",
         "2017-10-10T21:25:13.000Z",
         "2017-10-18T00:00:00.000Z"
        ],
        [
         "53cdb2fc8bc7dce0b6741e2150273451",
         "b0830fb4747a6c6d20dea0b8c802d7ef",
         "delivered",
         "2018-07-24T20:41:37.000Z",
         "2018-07-26T03:24:27.000Z",
         "2018-07-26T14:31:00.000Z",
         "2018-08-07T15:27:45.000Z",
         "2018-08-13T00:00:00.000Z"
        ],
        [
         "47770eb9100c2d0c44946d9cf07ec65d",
         "41ce2a54c0b03bf3443c3d931a367089",
         "delivered",
         "2018-08-08T08:38:49.000Z",
         "2018-08-08T08:55:23.000Z",
         "2018-08-08T13:50:00.000Z",
         "2018-08-17T18:06:29.000Z",
         "2018-09-04T00:00:00.000Z"
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "order_id",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "customer_id",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "order_status",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "order_purchase_timestamp",
         "type": "\"timestamp\""
        },
        {
         "metadata": "{}",
         "name": "order_approved_at",
         "type": "\"timestamp\""
        },
        {
         "metadata": "{}",
         "name": "order_delivered_carrier_date",
         "type": "\"timestamp\""
        },
        {
         "metadata": "{}",
         "name": "order_delivered_customer_date",
         "type": "\"timestamp\""
        },
        {
         "metadata": "{}",
         "name": "order_estimated_delivery_date",
         "type": "\"timestamp\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n\uD83D\uDCE6 === ORDER ITEMS (Sample) ===\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>order_id</th><th>order_item_id</th><th>product_id</th><th>seller_id</th><th>shipping_limit_date</th><th>price</th><th>freight_value</th></tr></thead><tbody><tr><td>00010242fe8c5a6d1ba2dd792cb16214</td><td>1</td><td>4244733e06e7ecb4970a6e2683c13e61</td><td>48436dade18ac8b2bce089ec2a041202</td><td>2017-09-19T09:45:35.000Z</td><td>58.9</td><td>13.29</td></tr><tr><td>00018f77f2f0320c557190d7a144bdd3</td><td>1</td><td>e5f2d52b802189ee658865ca93d83a8f</td><td>dd7ddc04e1b6c2c614352b383efe2d36</td><td>2017-05-03T11:05:13.000Z</td><td>239.9</td><td>19.93</td></tr><tr><td>000229ec398224ef6ca0657da4fc703e</td><td>1</td><td>c777355d18b72b67abbeef9df44fd0fd</td><td>5b51032eddd242adc84c38acab88f23d</td><td>2018-01-18T14:48:30.000Z</td><td>199.0</td><td>17.87</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "00010242fe8c5a6d1ba2dd792cb16214",
         1,
         "4244733e06e7ecb4970a6e2683c13e61",
         "48436dade18ac8b2bce089ec2a041202",
         "2017-09-19T09:45:35.000Z",
         58.9,
         13.29
        ],
        [
         "00018f77f2f0320c557190d7a144bdd3",
         1,
         "e5f2d52b802189ee658865ca93d83a8f",
         "dd7ddc04e1b6c2c614352b383efe2d36",
         "2017-05-03T11:05:13.000Z",
         239.9,
         19.93
        ],
        [
         "000229ec398224ef6ca0657da4fc703e",
         1,
         "c777355d18b72b67abbeef9df44fd0fd",
         "5b51032eddd242adc84c38acab88f23d",
         "2018-01-18T14:48:30.000Z",
         199.0,
         17.87
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "order_id",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "order_item_id",
         "type": "\"integer\""
        },
        {
         "metadata": "{}",
         "name": "product_id",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "seller_id",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "shipping_limit_date",
         "type": "\"timestamp\""
        },
        {
         "metadata": "{}",
         "name": "price",
         "type": "\"double\""
        },
        {
         "metadata": "{}",
         "name": "freight_value",
         "type": "\"double\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n\uD83D\uDCB3 === ORDER PAYMENTS (Sample) ===\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>order_id</th><th>payment_sequential</th><th>payment_type</th><th>payment_installments</th><th>payment_value</th></tr></thead><tbody><tr><td>b81ef226f3fe1789b1e8b2acac839d17</td><td>1</td><td>credit_card</td><td>8</td><td>99.33</td></tr><tr><td>a9810da82917af2d9aefd1278f1dcfa0</td><td>1</td><td>credit_card</td><td>1</td><td>24.39</td></tr><tr><td>25e8ea4e93396b6fa0d3dd708e76c1bd</td><td>1</td><td>credit_card</td><td>1</td><td>65.71</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "b81ef226f3fe1789b1e8b2acac839d17",
         1,
         "credit_card",
         8,
         99.33
        ],
        [
         "a9810da82917af2d9aefd1278f1dcfa0",
         1,
         "credit_card",
         1,
         24.39
        ],
        [
         "25e8ea4e93396b6fa0d3dd708e76c1bd",
         1,
         "credit_card",
         1,
         65.71
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "order_id",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "payment_sequential",
         "type": "\"integer\""
        },
        {
         "metadata": "{}",
         "name": "payment_type",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "payment_installments",
         "type": "\"integer\""
        },
        {
         "metadata": "{}",
         "name": "payment_value",
         "type": "\"double\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n⭐ === ORDER REVIEWS (Sample) ===\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>review_id</th><th>order_id</th><th>review_score</th><th>review_comment_title</th><th>review_comment_message</th><th>review_creation_date</th><th>review_answer_timestamp</th></tr></thead><tbody><tr><td>7bc2406110b926393aa56f80a40eba40</td><td>73fc7af87114b39712e6da79b0a377eb</td><td>4</td><td>null</td><td>null</td><td>2018-01-18 00:00:00</td><td>2018-01-18 21:46:59</td></tr><tr><td>80e641a11e56f04c1ad469d5645fdfde</td><td>a548910a1c6147796b98fdf73dbeba33</td><td>5</td><td>null</td><td>null</td><td>2018-03-10 00:00:00</td><td>2018-03-11 03:05:13</td></tr><tr><td>228ce5500dc1d8e020d8d1322874b6f0</td><td>f9e4b658b201a9f2ecdecbb34bed034b</td><td>5</td><td>null</td><td>null</td><td>2018-02-17 00:00:00</td><td>2018-02-18 14:36:24</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "7bc2406110b926393aa56f80a40eba40",
         "73fc7af87114b39712e6da79b0a377eb",
         "4",
         null,
         null,
         "2018-01-18 00:00:00",
         "2018-01-18 21:46:59"
        ],
        [
         "80e641a11e56f04c1ad469d5645fdfde",
         "a548910a1c6147796b98fdf73dbeba33",
         "5",
         null,
         null,
         "2018-03-10 00:00:00",
         "2018-03-11 03:05:13"
        ],
        [
         "228ce5500dc1d8e020d8d1322874b6f0",
         "f9e4b658b201a9f2ecdecbb34bed034b",
         "5",
         null,
         null,
         "2018-02-17 00:00:00",
         "2018-02-18 14:36:24"
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "review_id",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "order_id",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "review_score",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "review_comment_title",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "review_comment_message",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "review_creation_date",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "review_answer_timestamp",
         "type": "\"string\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n\uD83C\uDFF7️ === PRODUCTS (Sample) ===\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>product_id</th><th>product_category_name</th><th>product_name_lenght</th><th>product_description_lenght</th><th>product_photos_qty</th><th>product_weight_g</th><th>product_length_cm</th><th>product_height_cm</th><th>product_width_cm</th></tr></thead><tbody><tr><td>1e9e8ef04dbcff4541ed26657ea517e5</td><td>perfumaria</td><td>40</td><td>287</td><td>1</td><td>225</td><td>16</td><td>10</td><td>14</td></tr><tr><td>3aa071139cb16b67ca9e5dea641aaa2f</td><td>artes</td><td>44</td><td>276</td><td>1</td><td>1000</td><td>30</td><td>18</td><td>20</td></tr><tr><td>96bd76ec8810374ed1b65e291975717f</td><td>esporte_lazer</td><td>46</td><td>250</td><td>1</td><td>154</td><td>18</td><td>9</td><td>15</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "1e9e8ef04dbcff4541ed26657ea517e5",
         "perfumaria",
         40,
         287,
         1,
         225,
         16,
         10,
         14
        ],
        [
         "3aa071139cb16b67ca9e5dea641aaa2f",
         "artes",
         44,
         276,
         1,
         1000,
         30,
         18,
         20
        ],
        [
         "96bd76ec8810374ed1b65e291975717f",
         "esporte_lazer",
         46,
         250,
         1,
         154,
         18,
         9,
         15
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "product_id",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "product_category_name",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "product_name_lenght",
         "type": "\"integer\""
        },
        {
         "metadata": "{}",
         "name": "product_description_lenght",
         "type": "\"integer\""
        },
        {
         "metadata": "{}",
         "name": "product_photos_qty",
         "type": "\"integer\""
        },
        {
         "metadata": "{}",
         "name": "product_weight_g",
         "type": "\"integer\""
        },
        {
         "metadata": "{}",
         "name": "product_length_cm",
         "type": "\"integer\""
        },
        {
         "metadata": "{}",
         "name": "product_height_cm",
         "type": "\"integer\""
        },
        {
         "metadata": "{}",
         "name": "product_width_cm",
         "type": "\"integer\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n\uD83C\uDFEA === SELLERS (Sample) ===\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>seller_id</th><th>seller_zip_code_prefix</th><th>seller_city</th><th>seller_state</th></tr></thead><tbody><tr><td>3442f8959a84dea7ee197c632cb2df15</td><td>13023</td><td>campinas</td><td>SP</td></tr><tr><td>d1b65fc7debc3361ea86b5f14c68d2e2</td><td>13844</td><td>mogi guacu</td><td>SP</td></tr><tr><td>ce3ad9de960102d0677a81f5d0bb7b2d</td><td>20031</td><td>rio de janeiro</td><td>RJ</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "3442f8959a84dea7ee197c632cb2df15",
         13023,
         "campinas",
         "SP"
        ],
        [
         "d1b65fc7debc3361ea86b5f14c68d2e2",
         13844,
         "mogi guacu",
         "SP"
        ],
        [
         "ce3ad9de960102d0677a81f5d0bb7b2d",
         20031,
         "rio de janeiro",
         "RJ"
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "seller_id",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "seller_zip_code_prefix",
         "type": "\"integer\""
        },
        {
         "metadata": "{}",
         "name": "seller_city",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "seller_state",
         "type": "\"string\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n\uD83D\uDCCD === GEOLOCATION (Sample) ===\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>geolocation_zip_code_prefix</th><th>geolocation_lat</th><th>geolocation_lng</th><th>geolocation_city</th><th>geolocation_state</th></tr></thead><tbody><tr><td>1037</td><td>-23.54562128115268</td><td>-46.63929204800168</td><td>sao paulo</td><td>SP</td></tr><tr><td>1046</td><td>-23.546081127035535</td><td>-46.64482029837157</td><td>sao paulo</td><td>SP</td></tr><tr><td>1046</td><td>-23.54612896641469</td><td>-46.64295148361138</td><td>sao paulo</td><td>SP</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         1037,
         -23.54562128115268,
         -46.63929204800168,
         "sao paulo",
         "SP"
        ],
        [
         1046,
         -23.546081127035535,
         -46.64482029837157,
         "sao paulo",
         "SP"
        ],
        [
         1046,
         -23.54612896641469,
         -46.64295148361138,
         "sao paulo",
         "SP"
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "geolocation_zip_code_prefix",
         "type": "\"integer\""
        },
        {
         "metadata": "{}",
         "name": "geolocation_lat",
         "type": "\"double\""
        },
        {
         "metadata": "{}",
         "name": "geolocation_lng",
         "type": "\"double\""
        },
        {
         "metadata": "{}",
         "name": "geolocation_city",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "geolocation_state",
         "type": "\"string\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"\uD83D\uDD0D DATA EXPLORATION - SAMPLE ROWS FROM EACH TABLE\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# 1. CUSTOMERS\n",
    "print(\"\\n\uD83D\uDC65 === CUSTOMERS (Sample) ===\")\n",
    "display(df_customers_dataset.limit(3))\n",
    "\n",
    "# 2. ORDERS\n",
    "print(\"\\n\uD83D\uDED2 === ORDERS (Sample) ===\")\n",
    "display(df_orders.limit(3))\n",
    "\n",
    "# 3. ORDER ITEMS\n",
    "print(\"\\n\uD83D\uDCE6 === ORDER ITEMS (Sample) ===\")\n",
    "display(df_order_items.limit(3))\n",
    "\n",
    "# 4. ORDER PAYMENTS\n",
    "print(\"\\n\uD83D\uDCB3 === ORDER PAYMENTS (Sample) ===\")\n",
    "display(df_order_payments.limit(3))\n",
    "\n",
    "# 5. ORDER REVIEWS\n",
    "print(\"\\n⭐ === ORDER REVIEWS (Sample) ===\")\n",
    "display(df_order_reviews.limit(3))\n",
    "\n",
    "# 6. PRODUCTS\n",
    "print(\"\\n\uD83C\uDFF7️ === PRODUCTS (Sample) ===\")\n",
    "display(df_products.limit(3))\n",
    "\n",
    "# 7. SELLERS\n",
    "print(\"\\n\uD83C\uDFEA === SELLERS (Sample) ===\")\n",
    "display(df_sellers.limit(3))\n",
    "\n",
    "# 8. GEOLOCATION\n",
    "print(\"\\n\uD83D\uDCCD === GEOLOCATION (Sample) ===\")\n",
    "display(df_geolocation.limit(3))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c26cf515-c675-4e82-b7e4-5f0a3b7fc293",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n\uD83D\uDDD3️  CREATING DIM_DATE (Date Dimension)\n============================================================\n\uD83D\uDCC5 Generating 1461 dates from 2016-01-01 to 2019-12-31\n✅ DIM_DATE created with 1461 rows\n\n\uD83D\uDD0D Sample data:\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>date_key</th><th>full_date</th><th>year</th><th>quarter</th><th>month</th><th>month_name</th><th>day</th><th>day_of_week</th><th>day_name</th><th>week_of_year</th><th>is_weekend</th></tr></thead><tbody><tr><td>20160101</td><td>2016-01-01T00:00:00.000Z</td><td>2016</td><td>1</td><td>1</td><td>January</td><td>1</td><td>6</td><td>Friday</td><td>53</td><td>No</td></tr><tr><td>20160102</td><td>2016-01-02T00:00:00.000Z</td><td>2016</td><td>1</td><td>1</td><td>January</td><td>2</td><td>7</td><td>Saturday</td><td>53</td><td>Yes</td></tr><tr><td>20160103</td><td>2016-01-03T00:00:00.000Z</td><td>2016</td><td>1</td><td>1</td><td>January</td><td>3</td><td>1</td><td>Sunday</td><td>53</td><td>Yes</td></tr><tr><td>20160104</td><td>2016-01-04T00:00:00.000Z</td><td>2016</td><td>1</td><td>1</td><td>January</td><td>4</td><td>2</td><td>Monday</td><td>1</td><td>No</td></tr><tr><td>20160105</td><td>2016-01-05T00:00:00.000Z</td><td>2016</td><td>1</td><td>1</td><td>January</td><td>5</td><td>3</td><td>Tuesday</td><td>1</td><td>No</td></tr><tr><td>20160106</td><td>2016-01-06T00:00:00.000Z</td><td>2016</td><td>1</td><td>1</td><td>January</td><td>6</td><td>4</td><td>Wednesday</td><td>1</td><td>No</td></tr><tr><td>20160107</td><td>2016-01-07T00:00:00.000Z</td><td>2016</td><td>1</td><td>1</td><td>January</td><td>7</td><td>5</td><td>Thursday</td><td>1</td><td>No</td></tr><tr><td>20160108</td><td>2016-01-08T00:00:00.000Z</td><td>2016</td><td>1</td><td>1</td><td>January</td><td>8</td><td>6</td><td>Friday</td><td>1</td><td>No</td></tr><tr><td>20160109</td><td>2016-01-09T00:00:00.000Z</td><td>2016</td><td>1</td><td>1</td><td>January</td><td>9</td><td>7</td><td>Saturday</td><td>1</td><td>Yes</td></tr><tr><td>20160110</td><td>2016-01-10T00:00:00.000Z</td><td>2016</td><td>1</td><td>1</td><td>January</td><td>10</td><td>1</td><td>Sunday</td><td>1</td><td>Yes</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         20160101,
         "2016-01-01T00:00:00.000Z",
         2016,
         1,
         1,
         "January",
         1,
         6,
         "Friday",
         53,
         "No"
        ],
        [
         20160102,
         "2016-01-02T00:00:00.000Z",
         2016,
         1,
         1,
         "January",
         2,
         7,
         "Saturday",
         53,
         "Yes"
        ],
        [
         20160103,
         "2016-01-03T00:00:00.000Z",
         2016,
         1,
         1,
         "January",
         3,
         1,
         "Sunday",
         53,
         "Yes"
        ],
        [
         20160104,
         "2016-01-04T00:00:00.000Z",
         2016,
         1,
         1,
         "January",
         4,
         2,
         "Monday",
         1,
         "No"
        ],
        [
         20160105,
         "2016-01-05T00:00:00.000Z",
         2016,
         1,
         1,
         "January",
         5,
         3,
         "Tuesday",
         1,
         "No"
        ],
        [
         20160106,
         "2016-01-06T00:00:00.000Z",
         2016,
         1,
         1,
         "January",
         6,
         4,
         "Wednesday",
         1,
         "No"
        ],
        [
         20160107,
         "2016-01-07T00:00:00.000Z",
         2016,
         1,
         1,
         "January",
         7,
         5,
         "Thursday",
         1,
         "No"
        ],
        [
         20160108,
         "2016-01-08T00:00:00.000Z",
         2016,
         1,
         1,
         "January",
         8,
         6,
         "Friday",
         1,
         "No"
        ],
        [
         20160109,
         "2016-01-09T00:00:00.000Z",
         2016,
         1,
         1,
         "January",
         9,
         7,
         "Saturday",
         1,
         "Yes"
        ],
        [
         20160110,
         "2016-01-10T00:00:00.000Z",
         2016,
         1,
         1,
         "January",
         10,
         1,
         "Sunday",
         1,
         "Yes"
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "date_key",
         "type": "\"integer\""
        },
        {
         "metadata": "{}",
         "name": "full_date",
         "type": "\"timestamp\""
        },
        {
         "metadata": "{}",
         "name": "year",
         "type": "\"integer\""
        },
        {
         "metadata": "{}",
         "name": "quarter",
         "type": "\"integer\""
        },
        {
         "metadata": "{}",
         "name": "month",
         "type": "\"integer\""
        },
        {
         "metadata": "{}",
         "name": "month_name",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "day",
         "type": "\"integer\""
        },
        {
         "metadata": "{}",
         "name": "day_of_week",
         "type": "\"integer\""
        },
        {
         "metadata": "{}",
         "name": "day_name",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "week_of_year",
         "type": "\"integer\""
        },
        {
         "metadata": "{}",
         "name": "is_weekend",
         "type": "\"string\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n\uD83D\uDCCA Date range coverage:\n+-------------------+\n|     min(full_date)|\n+-------------------+\n|2016-01-01 00:00:00|\n+-------------------+\n\n+-------------------+\n|     max(full_date)|\n+-------------------+\n|2019-12-31 00:00:00|\n+-------------------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, date_format, dayofweek, dayofmonth, dayofyear, weekofyear, month, quarter, year, when\n",
    "from pyspark.sql.types import DateType\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"\uD83D\uDDD3️  CREATING DIM_DATE (Date Dimension)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Step 1: Define date range for our data\n",
    "# We'll generate dates from 2016-01-01 to 2019-12-31 (covers our e-commerce data)\n",
    "\n",
    "start_date = datetime(2016, 1, 1)\n",
    "end_date = datetime(2019, 12, 31)\n",
    "\n",
    "# Step 2: Generate list of dates\n",
    "date_list = []\n",
    "current_date = start_date\n",
    "\n",
    "while current_date <= end_date:\n",
    "    date_list.append((current_date,))\n",
    "    current_date += timedelta(days=1)\n",
    "\n",
    "print(f\"\uD83D\uDCC5 Generating {len(date_list)} dates from {start_date.date()} to {end_date.date()}\")\n",
    "\n",
    "# Step 3: Create DataFrame with dates\n",
    "dim_date = spark.createDataFrame(date_list, [\"full_date\"])\n",
    "\n",
    "# Step 4: Add all time attributes\n",
    "dim_date = dim_date.withColumn(\"date_key\", date_format(col(\"full_date\"), \"yyyyMMdd\").cast(\"int\")) \\\n",
    "    .withColumn(\"year\", year(col(\"full_date\"))) \\\n",
    "    .withColumn(\"quarter\", quarter(col(\"full_date\"))) \\\n",
    "    .withColumn(\"month\", month(col(\"full_date\"))) \\\n",
    "    .withColumn(\"month_name\", date_format(col(\"full_date\"), \"MMMM\")) \\\n",
    "    .withColumn(\"day\", dayofmonth(col(\"full_date\"))) \\\n",
    "    .withColumn(\"day_of_week\", dayofweek(col(\"full_date\"))) \\\n",
    "    .withColumn(\"day_name\", date_format(col(\"full_date\"), \"EEEE\")) \\\n",
    "    .withColumn(\"week_of_year\", weekofyear(col(\"full_date\"))) \\\n",
    "    .withColumn(\"is_weekend\", when(dayofweek(col(\"full_date\")).isin([1, 7]), \"Yes\").otherwise(\"No\"))\n",
    "\n",
    "# Step 5: Reorder columns for better readability\n",
    "dim_date = dim_date.select(\n",
    "    \"date_key\",\n",
    "    \"full_date\",\n",
    "    \"year\",\n",
    "    \"quarter\",\n",
    "    \"month\",\n",
    "    \"month_name\",\n",
    "    \"day\",\n",
    "    \"day_of_week\",\n",
    "    \"day_name\",\n",
    "    \"week_of_year\",\n",
    "    \"is_weekend\"\n",
    ")\n",
    "\n",
    "print(f\"✅ DIM_DATE created with {dim_date.count()} rows\")\n",
    "print(\"\\n\uD83D\uDD0D Sample data:\")\n",
    "display(dim_date.limit(10))\n",
    "\n",
    "print(\"\\n\uD83D\uDCCA Date range coverage:\")\n",
    "dim_date.agg({\"full_date\": \"min\"}).show()\n",
    "dim_date.agg({\"full_date\": \"max\"}).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "42f81bf1-3196-4c3d-a087-52ef0fb36c00",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n\uD83D\uDC65 CREATING DIM_CUSTOMER (Customer Dimension)\n============================================================\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/databricks/python/lib/python3.12/site-packages/pyspark/sql/connect/expressions.py:1134: UserWarning: WARN WindowExpression: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n  warnings.warn(\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ DIM_CUSTOMER created with 99,441 rows\n\n\uD83D\uDD0D Sample data:\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>customer_key</th><th>customer_id</th><th>customer_unique_id</th><th>customer_zip_code</th><th>customer_city</th><th>customer_state</th><th>customer_segment</th></tr></thead><tbody><tr><td>1</td><td>00012a2ce6f8dcda20d059ce98491703</td><td>248ffe10d632bebe4f7267f1f44844c9</td><td>6273</td><td>osasco</td><td>SP</td><td>Metro</td></tr><tr><td>2</td><td>000161a058600d5901f007fab4c27140</td><td>b0015e09bb4b6e47c52844fab5fb6638</td><td>35550</td><td>itapecerica</td><td>MG</td><td>Tier-1</td></tr><tr><td>3</td><td>0001fd6190edaaf884bcaf3d49edf079</td><td>94b11d37cd61cb2994a194d11f89682b</td><td>29830</td><td>nova venecia</td><td>ES</td><td>Tier-2</td></tr><tr><td>4</td><td>0002414f95344307404f0ace7a26f1d5</td><td>4893ad4ea28b2c5b3ddf4e82e79db9e6</td><td>39664</td><td>mendonca</td><td>MG</td><td>Tier-1</td></tr><tr><td>5</td><td>000379cdec625522490c315e70c7a9fb</td><td>0b83f73b19c2019e182fd552c048a22c</td><td>4841</td><td>sao paulo</td><td>SP</td><td>Metro</td></tr><tr><td>6</td><td>0004164d20a9e969af783496f3408652</td><td>104bdb7e6a6cdceaa88c3ea5fa6b2b93</td><td>13272</td><td>valinhos</td><td>SP</td><td>Metro</td></tr><tr><td>7</td><td>000419c5494106c306a97b5635748086</td><td>14843983d4a159080f6afe4b7f346e7c</td><td>24220</td><td>niteroi</td><td>RJ</td><td>Metro</td></tr><tr><td>8</td><td>00046a560d407e99b969756e0b10f282</td><td>0b5295fc9819d831f68eb0e9a3e13ab7</td><td>20540</td><td>rio de janeiro</td><td>RJ</td><td>Metro</td></tr><tr><td>9</td><td>00050bf6e01e69d5c0fd612f1bcfb69c</td><td>e3cf594a99e810f58af53ed4820f25e5</td><td>98700</td><td>ijui</td><td>RS</td><td>Tier-1</td></tr><tr><td>10</td><td>000598caf2ef4117407665ac33275130</td><td>7e0516b486e92ed3f3afdd6d1276cfbd</td><td>35540</td><td>oliveira</td><td>MG</td><td>Tier-1</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         1,
         "00012a2ce6f8dcda20d059ce98491703",
         "248ffe10d632bebe4f7267f1f44844c9",
         6273,
         "osasco",
         "SP",
         "Metro"
        ],
        [
         2,
         "000161a058600d5901f007fab4c27140",
         "b0015e09bb4b6e47c52844fab5fb6638",
         35550,
         "itapecerica",
         "MG",
         "Tier-1"
        ],
        [
         3,
         "0001fd6190edaaf884bcaf3d49edf079",
         "94b11d37cd61cb2994a194d11f89682b",
         29830,
         "nova venecia",
         "ES",
         "Tier-2"
        ],
        [
         4,
         "0002414f95344307404f0ace7a26f1d5",
         "4893ad4ea28b2c5b3ddf4e82e79db9e6",
         39664,
         "mendonca",
         "MG",
         "Tier-1"
        ],
        [
         5,
         "000379cdec625522490c315e70c7a9fb",
         "0b83f73b19c2019e182fd552c048a22c",
         4841,
         "sao paulo",
         "SP",
         "Metro"
        ],
        [
         6,
         "0004164d20a9e969af783496f3408652",
         "104bdb7e6a6cdceaa88c3ea5fa6b2b93",
         13272,
         "valinhos",
         "SP",
         "Metro"
        ],
        [
         7,
         "000419c5494106c306a97b5635748086",
         "14843983d4a159080f6afe4b7f346e7c",
         24220,
         "niteroi",
         "RJ",
         "Metro"
        ],
        [
         8,
         "00046a560d407e99b969756e0b10f282",
         "0b5295fc9819d831f68eb0e9a3e13ab7",
         20540,
         "rio de janeiro",
         "RJ",
         "Metro"
        ],
        [
         9,
         "00050bf6e01e69d5c0fd612f1bcfb69c",
         "e3cf594a99e810f58af53ed4820f25e5",
         98700,
         "ijui",
         "RS",
         "Tier-1"
        ],
        [
         10,
         "000598caf2ef4117407665ac33275130",
         "7e0516b486e92ed3f3afdd6d1276cfbd",
         35540,
         "oliveira",
         "MG",
         "Tier-1"
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "customer_key",
         "type": "\"integer\""
        },
        {
         "metadata": "{}",
         "name": "customer_id",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "customer_unique_id",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "customer_zip_code",
         "type": "\"integer\""
        },
        {
         "metadata": "{}",
         "name": "customer_city",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "customer_state",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "customer_segment",
         "type": "\"string\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n\uD83D\uDCCA Customer Distribution by State:\n+--------------+-----+\n|customer_state|count|\n+--------------+-----+\n|            SP|41746|\n|            RJ|12852|\n|            MG|11635|\n|            RS| 5466|\n|            PR| 5045|\n|            SC| 3637|\n|            BA| 3380|\n|            DF| 2140|\n|            ES| 2033|\n|            GO| 2020|\n+--------------+-----+\nonly showing top 10 rows\n\n\uD83D\uDCCA Customer Segments:\n+----------------+-----+\n|customer_segment|count|\n+----------------+-----+\n|          Tier-2|22697|\n|          Tier-1|22146|\n|           Metro|54598|\n+----------------+-----+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import row_number, monotonically_increasing_id\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"\uD83D\uDC65 CREATING DIM_CUSTOMER (Customer Dimension)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Step 1: Start with YOUR customers dataset\n",
    "dim_customer = df_customers_dataset  # ← Updated to your variable name\n",
    "\n",
    "# Step 2: Add surrogate key\n",
    "dim_customer = dim_customer.withColumn(\n",
    "    \"customer_key\",\n",
    "    row_number().over(Window.orderBy(\"customer_id\"))\n",
    ")\n",
    "\n",
    "# Step 3: Select and rename columns\n",
    "dim_customer = dim_customer.select(\n",
    "    col(\"customer_key\"),\n",
    "    col(\"customer_id\"),\n",
    "    col(\"customer_unique_id\"),\n",
    "    col(\"customer_zip_code_prefix\").alias(\"customer_zip_code\"),\n",
    "    col(\"customer_city\"),\n",
    "    col(\"customer_state\")\n",
    ")\n",
    "\n",
    "# Step 4: Add customer segment\n",
    "dim_customer = dim_customer.withColumn(\n",
    "    \"customer_segment\",\n",
    "    when(col(\"customer_state\").isin([\"SP\", \"RJ\"]), \"Metro\")\n",
    "    .when(col(\"customer_state\").isin([\"MG\", \"RS\", \"PR\"]), \"Tier-1\")\n",
    "    .otherwise(\"Tier-2\")\n",
    ")\n",
    "\n",
    "print(f\"✅ DIM_CUSTOMER created with {dim_customer.count():,} rows\")\n",
    "print(\"\\n\uD83D\uDD0D Sample data:\")\n",
    "display(dim_customer.limit(10))\n",
    "\n",
    "print(\"\\n\uD83D\uDCCA Customer Distribution by State:\")\n",
    "dim_customer.groupBy(\"customer_state\").count().orderBy(col(\"count\").desc()).show(10)\n",
    "\n",
    "print(\"\\n\uD83D\uDCCA Customer Segments:\")\n",
    "dim_customer.groupBy(\"customer_segment\").count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "aafb7a69-34e0-48ee-a657-322a3d1264a3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n\uD83C\uDFF7️  CREATING DIM_PRODUCT (Product Dimension)\n============================================================\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/databricks/python/lib/python3.12/site-packages/pyspark/sql/connect/expressions.py:1134: UserWarning: WARN WindowExpression: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n  warnings.warn(\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ DIM_PRODUCT created with 32,951 rows\n\n\uD83D\uDD0D Sample data:\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>product_key</th><th>product_id</th><th>product_category_name</th><th>product_weight_g</th><th>product_length_cm</th><th>product_height_cm</th><th>product_width_cm</th><th>product_volume_cm3</th><th>product_photos_qty</th></tr></thead><tbody><tr><td>1</td><td>00066f42aeeb9f3007548bb9d3f33c38</td><td>perfumaria</td><td>300</td><td>20</td><td>16</td><td>16</td><td>5120</td><td>6</td></tr><tr><td>2</td><td>00088930e925c41fd95ebfe695fd2655</td><td>automotivo</td><td>1225</td><td>55</td><td>10</td><td>26</td><td>14300</td><td>4</td></tr><tr><td>3</td><td>0009406fd7479715e4bef61dd91f2462</td><td>cama_mesa_banho</td><td>300</td><td>45</td><td>15</td><td>35</td><td>23625</td><td>2</td></tr><tr><td>4</td><td>000b8f95fcb9e0096488278317764d19</td><td>utilidades_domesticas</td><td>550</td><td>19</td><td>24</td><td>12</td><td>5472</td><td>3</td></tr><tr><td>5</td><td>000d9be29b5207b54e86aa1b1ac54872</td><td>relogios_presentes</td><td>250</td><td>22</td><td>11</td><td>15</td><td>3630</td><td>4</td></tr><tr><td>6</td><td>0011c512eb256aa0dbbb544d8dffcf6e</td><td>automotivo</td><td>100</td><td>16</td><td>15</td><td>16</td><td>3840</td><td>1</td></tr><tr><td>7</td><td>00126f27c813603687e6ce486d909d01</td><td>cool_stuff</td><td>700</td><td>25</td><td>5</td><td>15</td><td>1875</td><td>1</td></tr><tr><td>8</td><td>001795ec6f1b187d37335e1c4704762e</td><td>consoles_games</td><td>600</td><td>30</td><td>20</td><td>20</td><td>12000</td><td>1</td></tr><tr><td>9</td><td>001b237c0e9bb435f2e54071129237e9</td><td>cama_mesa_banho</td><td>6000</td><td>40</td><td>4</td><td>30</td><td>4800</td><td>1</td></tr><tr><td>10</td><td>001b72dfd63e9833e8c02742adf472e3</td><td>moveis_decoracao</td><td>600</td><td>26</td><td>8</td><td>22</td><td>4576</td><td>3</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         1,
         "00066f42aeeb9f3007548bb9d3f33c38",
         "perfumaria",
         300,
         20,
         16,
         16,
         5120,
         6
        ],
        [
         2,
         "00088930e925c41fd95ebfe695fd2655",
         "automotivo",
         1225,
         55,
         10,
         26,
         14300,
         4
        ],
        [
         3,
         "0009406fd7479715e4bef61dd91f2462",
         "cama_mesa_banho",
         300,
         45,
         15,
         35,
         23625,
         2
        ],
        [
         4,
         "000b8f95fcb9e0096488278317764d19",
         "utilidades_domesticas",
         550,
         19,
         24,
         12,
         5472,
         3
        ],
        [
         5,
         "000d9be29b5207b54e86aa1b1ac54872",
         "relogios_presentes",
         250,
         22,
         11,
         15,
         3630,
         4
        ],
        [
         6,
         "0011c512eb256aa0dbbb544d8dffcf6e",
         "automotivo",
         100,
         16,
         15,
         16,
         3840,
         1
        ],
        [
         7,
         "00126f27c813603687e6ce486d909d01",
         "cool_stuff",
         700,
         25,
         5,
         15,
         1875,
         1
        ],
        [
         8,
         "001795ec6f1b187d37335e1c4704762e",
         "consoles_games",
         600,
         30,
         20,
         20,
         12000,
         1
        ],
        [
         9,
         "001b237c0e9bb435f2e54071129237e9",
         "cama_mesa_banho",
         6000,
         40,
         4,
         30,
         4800,
         1
        ],
        [
         10,
         "001b72dfd63e9833e8c02742adf472e3",
         "moveis_decoracao",
         600,
         26,
         8,
         22,
         4576,
         3
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "product_key",
         "type": "\"integer\""
        },
        {
         "metadata": "{}",
         "name": "product_id",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "product_category_name",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "product_weight_g",
         "type": "\"integer\""
        },
        {
         "metadata": "{}",
         "name": "product_length_cm",
         "type": "\"integer\""
        },
        {
         "metadata": "{}",
         "name": "product_height_cm",
         "type": "\"integer\""
        },
        {
         "metadata": "{}",
         "name": "product_width_cm",
         "type": "\"integer\""
        },
        {
         "metadata": "{}",
         "name": "product_volume_cm3",
         "type": "\"integer\""
        },
        {
         "metadata": "{}",
         "name": "product_photos_qty",
         "type": "\"integer\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n\uD83D\uDCCA Top 10 Product Categories:\n+---------------------+-----+\n|product_category_name|count|\n+---------------------+-----+\n|      cama_mesa_banho| 3029|\n|        esporte_lazer| 2867|\n|     moveis_decoracao| 2657|\n|         beleza_saude| 2444|\n| utilidades_domest...| 2335|\n|           automotivo| 1900|\n| informatica_acess...| 1639|\n|           brinquedos| 1411|\n|   relogios_presentes| 1329|\n|            telefonia| 1134|\n+---------------------+-----+\nonly showing top 10 rows\n"
     ]
    }
   ],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"\uD83C\uDFF7️  CREATING DIM_PRODUCT (Product Dimension)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Step 1: Start with YOUR products dataset\n",
    "dim_product = df_products  # ← Your variable name\n",
    "\n",
    "# Step 2: Add surrogate key\n",
    "dim_product = dim_product.withColumn(\n",
    "    \"product_key\",\n",
    "    row_number().over(Window.orderBy(\"product_id\"))\n",
    ")\n",
    "\n",
    "# Step 3: Calculate product volume\n",
    "dim_product = dim_product.withColumn(\n",
    "    \"product_volume_cm3\",\n",
    "    col(\"product_length_cm\") * col(\"product_height_cm\") * col(\"product_width_cm\")\n",
    ")\n",
    "\n",
    "# Step 4: Select columns\n",
    "dim_product = dim_product.select(\n",
    "    col(\"product_key\"),\n",
    "    col(\"product_id\"),\n",
    "    col(\"product_category_name\"),\n",
    "    col(\"product_weight_g\"),\n",
    "    col(\"product_length_cm\"),\n",
    "    col(\"product_height_cm\"),\n",
    "    col(\"product_width_cm\"),\n",
    "    col(\"product_volume_cm3\"),\n",
    "    col(\"product_photos_qty\")\n",
    ")\n",
    "\n",
    "# Step 5: Handle NULL category names\n",
    "dim_product = dim_product.withColumn(\n",
    "    \"product_category_name\",\n",
    "    when(col(\"product_category_name\").isNull(), \"Unknown\").otherwise(col(\"product_category_name\"))\n",
    ")\n",
    "\n",
    "print(f\"✅ DIM_PRODUCT created with {dim_product.count():,} rows\")\n",
    "print(\"\\n\uD83D\uDD0D Sample data:\")\n",
    "display(dim_product.limit(10))\n",
    "\n",
    "print(\"\\n\uD83D\uDCCA Top 10 Product Categories:\")\n",
    "dim_product.groupBy(\"product_category_name\").count().orderBy(col(\"count\").desc()).show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "38dd248c-d1d2-471d-a77a-f6a881913919",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n\uD83C\uDFEA CREATING DIM_SELLER (Seller Dimension)\n============================================================\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/databricks/python/lib/python3.12/site-packages/pyspark/sql/connect/expressions.py:1134: UserWarning: WARN WindowExpression: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n  warnings.warn(\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ DIM_SELLER created with 3,095 rows\n\n\uD83D\uDD0D Sample data:\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>seller_key</th><th>seller_id</th><th>seller_zip_code</th><th>seller_city</th><th>seller_state</th></tr></thead><tbody><tr><td>1</td><td>0015a82c2db000af6aaaf3ae2ecb0532</td><td>9080</td><td>santo andre</td><td>SP</td></tr><tr><td>2</td><td>001cca7ae9ae17fb1caed9dfb1094831</td><td>29156</td><td>cariacica</td><td>ES</td></tr><tr><td>3</td><td>001e6ad469a905060d959994f1b41e4f</td><td>24754</td><td>sao goncalo</td><td>RJ</td></tr><tr><td>4</td><td>002100f778ceb8431b7a1020ff7ab48f</td><td>14405</td><td>franca</td><td>SP</td></tr><tr><td>5</td><td>003554e2dce176b5555353e4f3555ac8</td><td>74565</td><td>goiania</td><td>GO</td></tr><tr><td>6</td><td>004c9cd9d87a3c30c522c48c4fc07416</td><td>14940</td><td>ibitinga</td><td>SP</td></tr><tr><td>7</td><td>00720abe85ba0859807595bbf045a33b</td><td>7070</td><td>guarulhos</td><td>SP</td></tr><tr><td>8</td><td>00ab3eff1b5192e5f1a63bcecfee11c8</td><td>4164</td><td>sao paulo</td><td>SP</td></tr><tr><td>9</td><td>00d8b143d12632bad99c0ad66ad52825</td><td>30170</td><td>belo horizonte</td><td>MG</td></tr><tr><td>10</td><td>00ee68308b45bc5e2660cd833c3f81cc</td><td>3333</td><td>sao paulo</td><td>SP</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         1,
         "0015a82c2db000af6aaaf3ae2ecb0532",
         9080,
         "santo andre",
         "SP"
        ],
        [
         2,
         "001cca7ae9ae17fb1caed9dfb1094831",
         29156,
         "cariacica",
         "ES"
        ],
        [
         3,
         "001e6ad469a905060d959994f1b41e4f",
         24754,
         "sao goncalo",
         "RJ"
        ],
        [
         4,
         "002100f778ceb8431b7a1020ff7ab48f",
         14405,
         "franca",
         "SP"
        ],
        [
         5,
         "003554e2dce176b5555353e4f3555ac8",
         74565,
         "goiania",
         "GO"
        ],
        [
         6,
         "004c9cd9d87a3c30c522c48c4fc07416",
         14940,
         "ibitinga",
         "SP"
        ],
        [
         7,
         "00720abe85ba0859807595bbf045a33b",
         7070,
         "guarulhos",
         "SP"
        ],
        [
         8,
         "00ab3eff1b5192e5f1a63bcecfee11c8",
         4164,
         "sao paulo",
         "SP"
        ],
        [
         9,
         "00d8b143d12632bad99c0ad66ad52825",
         30170,
         "belo horizonte",
         "MG"
        ],
        [
         10,
         "00ee68308b45bc5e2660cd833c3f81cc",
         3333,
         "sao paulo",
         "SP"
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "seller_key",
         "type": "\"integer\""
        },
        {
         "metadata": "{}",
         "name": "seller_id",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "seller_zip_code",
         "type": "\"integer\""
        },
        {
         "metadata": "{}",
         "name": "seller_city",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "seller_state",
         "type": "\"string\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n\uD83D\uDCCA Sellers by State:\n+------------+-----+\n|seller_state|count|\n+------------+-----+\n|          SP| 1849|\n|          PR|  349|\n|          MG|  244|\n|          SC|  190|\n|          RJ|  171|\n|          RS|  129|\n|          GO|   40|\n|          DF|   30|\n|          ES|   23|\n|          BA|   19|\n+------------+-----+\nonly showing top 10 rows\n"
     ]
    }
   ],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"\uD83C\uDFEA CREATING DIM_SELLER (Seller Dimension)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Step 1: Start with YOUR sellers dataset\n",
    "dim_seller = df_sellers  # ← Your variable name\n",
    "\n",
    "# Step 2: Add surrogate key\n",
    "dim_seller = dim_seller.withColumn(\n",
    "    \"seller_key\",\n",
    "    row_number().over(Window.orderBy(\"seller_id\"))\n",
    ")\n",
    "\n",
    "# Step 3: Select columns\n",
    "dim_seller = dim_seller.select(\n",
    "    col(\"seller_key\"),\n",
    "    col(\"seller_id\"),\n",
    "    col(\"seller_zip_code_prefix\").alias(\"seller_zip_code\"),\n",
    "    col(\"seller_city\"),\n",
    "    col(\"seller_state\")\n",
    ")\n",
    "\n",
    "print(f\"✅ DIM_SELLER created with {dim_seller.count():,} rows\")\n",
    "print(\"\\n\uD83D\uDD0D Sample data:\")\n",
    "display(dim_seller.limit(10))\n",
    "\n",
    "print(\"\\n\uD83D\uDCCA Sellers by State:\")\n",
    "dim_seller.groupBy(\"seller_state\").count().orderBy(col(\"count\").desc()).show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "96d7c67e-5117-4ef2-b839-0de266d7498b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n\uD83D\uDCCA DIMENSION TABLES SUMMARY\n============================================================\n✅ DIM_DATE: 1,461 rows\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/databricks/python/lib/python3.12/site-packages/pyspark/sql/connect/expressions.py:1134: UserWarning: WARN WindowExpression: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n  warnings.warn(\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ DIM_CUSTOMER: 99,441 rows\n✅ DIM_PRODUCT: 32,951 rows\n✅ DIM_SELLER: 3,095 rows\n============================================================\n\n\uD83C\uDF89 All dimension tables created successfully!\nReady to create FACT_SALES table!\n"
     ]
    }
   ],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"\uD83D\uDCCA DIMENSION TABLES SUMMARY\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"✅ DIM_DATE: {dim_date.count():,} rows\")\n",
    "print(f\"✅ DIM_CUSTOMER: {dim_customer.count():,} rows\")\n",
    "print(f\"✅ DIM_PRODUCT: {dim_product.count():,} rows\")\n",
    "print(f\"✅ DIM_SELLER: {dim_seller.count():,} rows\")\n",
    "print(\"=\" * 60)\n",
    "print(\"\\n\uD83C\uDF89 All dimension tables created successfully!\")\n",
    "print(\"Ready to create FACT_SALES table!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8068f27f-c809-41af-b0e8-a423cb1d42ba",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n\uD83C\uDF1F CREATING FACT_SALES (The Heart of Star Schema!)\n============================================================\n\n\uD83D\uDCE6 Step 1: Starting with ORDER ITEMS as base...\n\uD83D\uDCE6 Step 2: Joining with ORDERS...\n\uD83D\uDCE6 Step 3: Joining with ORDER PAYMENTS...\n\uD83D\uDCE6 Step 4: Joining with ORDER REVIEWS...\n\uD83D\uDCE6 Step 5: Joining with DIM_CUSTOMER to get customer_key...\n\uD83D\uDCE6 Step 6: Joining with DIM_PRODUCT to get product_key...\n\uD83D\uDCE6 Step 7: Joining with DIM_SELLER to get seller_key...\n\uD83D\uDCE6 Step 8: Joining with DIM_DATE to get date_key...\n\uD83D\uDCE6 Step 9: Creating sale_key (surrogate key)...\n\uD83D\uDCE6 Step 10: Calculating total_amount...\n\uD83D\uDCE6 Step 11: Selecting final columns...\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/databricks/python/lib/python3.12/site-packages/pyspark/sql/connect/expressions.py:1134: UserWarning: WARN WindowExpression: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n  warnings.warn(\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n✅ FACT_SALES created with 113,314 rows\n\n\uD83D\uDD0D Sample data:\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>sale_key</th><th>order_id</th><th>order_item_id</th><th>customer_key</th><th>product_key</th><th>seller_key</th><th>date_key</th><th>price</th><th>freight_value</th><th>total_amount</th><th>payment_value</th><th>payment_installments</th><th>review_score</th></tr></thead><tbody><tr><td>1</td><td>00010242fe8c5a6d1ba2dd792cb16214</td><td>1</td><td>23675</td><td>8629</td><td>855</td><td>20170913</td><td>58.90</td><td>13.29</td><td>72.19</td><td>72.19</td><td>2</td><td>5</td></tr><tr><td>2</td><td>00018f77f2f0320c557190d7a144bdd3</td><td>1</td><td>95963</td><td>29598</td><td>2679</td><td>20170426</td><td>239.90</td><td>19.93</td><td>259.83</td><td>259.83</td><td>3</td><td>4</td></tr><tr><td>3</td><td>000229ec398224ef6ca0657da4fc703e</td><td>1</td><td>38903</td><td>25668</td><td>1118</td><td>20180114</td><td>199.00</td><td>17.87</td><td>216.87</td><td>216.87</td><td>5</td><td>5</td></tr><tr><td>4</td><td>00024acbcdf0a6daa1e931b038114c75</td><td>1</td><td>82958</td><td>15323</td><td>1920</td><td>20180808</td><td>12.99</td><td>12.79</td><td>25.78</td><td>25.78</td><td>2</td><td>4</td></tr><tr><td>5</td><td>00042b26cf59d7ce69dfabb4e55b4fd9</td><td>1</td><td>34382</td><td>22080</td><td>2698</td><td>20170204</td><td>199.90</td><td>18.14</td><td>218.04</td><td>218.04</td><td>3</td><td>5</td></tr><tr><td>6</td><td>00048cc3ae777c65dbb7d2a0634bc1ea</td><td>1</td><td>50184</td><td>30848</td><td>1224</td><td>20170515</td><td>21.90</td><td>12.69</td><td>34.59</td><td>34.59</td><td>1</td><td>4</td></tr><tr><td>7</td><td>00054e8431b9d7675808bcb819fb4a32</td><td>1</td><td>19737</td><td>18182</td><td>1372</td><td>20171210</td><td>19.90</td><td>11.85</td><td>31.75</td><td>31.75</td><td>1</td><td>4</td></tr><tr><td>8</td><td>000576fe39319847cbb9d288c5617fa6</td><td>1</td><td>61707</td><td>11123</td><td>1091</td><td>20180704</td><td>810.00</td><td>70.75</td><td>880.75</td><td>880.75</td><td>10</td><td>5</td></tr><tr><td>9</td><td>0005a1a1728c9d785b8e2b08b904576c</td><td>1</td><td>8540</td><td>6385</td><td>1997</td><td>20180319</td><td>145.95</td><td>11.65</td><td>157.60</td><td>157.60</td><td>3</td><td>1</td></tr><tr><td>10</td><td>0005f50442cb953dcd1d21e1fb923495</td><td>1</td><td>20619</td><td>9013</td><td>2231</td><td>20180702</td><td>53.99</td><td>11.40</td><td>65.39</td><td>65.39</td><td>1</td><td>4</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         1,
         "00010242fe8c5a6d1ba2dd792cb16214",
         1,
         23675,
         8629,
         855,
         20170913,
         "58.90",
         "13.29",
         "72.19",
         "72.19",
         2,
         5
        ],
        [
         2,
         "00018f77f2f0320c557190d7a144bdd3",
         1,
         95963,
         29598,
         2679,
         20170426,
         "239.90",
         "19.93",
         "259.83",
         "259.83",
         3,
         4
        ],
        [
         3,
         "000229ec398224ef6ca0657da4fc703e",
         1,
         38903,
         25668,
         1118,
         20180114,
         "199.00",
         "17.87",
         "216.87",
         "216.87",
         5,
         5
        ],
        [
         4,
         "00024acbcdf0a6daa1e931b038114c75",
         1,
         82958,
         15323,
         1920,
         20180808,
         "12.99",
         "12.79",
         "25.78",
         "25.78",
         2,
         4
        ],
        [
         5,
         "00042b26cf59d7ce69dfabb4e55b4fd9",
         1,
         34382,
         22080,
         2698,
         20170204,
         "199.90",
         "18.14",
         "218.04",
         "218.04",
         3,
         5
        ],
        [
         6,
         "00048cc3ae777c65dbb7d2a0634bc1ea",
         1,
         50184,
         30848,
         1224,
         20170515,
         "21.90",
         "12.69",
         "34.59",
         "34.59",
         1,
         4
        ],
        [
         7,
         "00054e8431b9d7675808bcb819fb4a32",
         1,
         19737,
         18182,
         1372,
         20171210,
         "19.90",
         "11.85",
         "31.75",
         "31.75",
         1,
         4
        ],
        [
         8,
         "000576fe39319847cbb9d288c5617fa6",
         1,
         61707,
         11123,
         1091,
         20180704,
         "810.00",
         "70.75",
         "880.75",
         "880.75",
         10,
         5
        ],
        [
         9,
         "0005a1a1728c9d785b8e2b08b904576c",
         1,
         8540,
         6385,
         1997,
         20180319,
         "145.95",
         "11.65",
         "157.60",
         "157.60",
         3,
         1
        ],
        [
         10,
         "0005f50442cb953dcd1d21e1fb923495",
         1,
         20619,
         9013,
         2231,
         20180702,
         "53.99",
         "11.40",
         "65.39",
         "65.39",
         1,
         4
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "sale_key",
         "type": "\"integer\""
        },
        {
         "metadata": "{}",
         "name": "order_id",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "order_item_id",
         "type": "\"integer\""
        },
        {
         "metadata": "{}",
         "name": "customer_key",
         "type": "\"integer\""
        },
        {
         "metadata": "{}",
         "name": "product_key",
         "type": "\"integer\""
        },
        {
         "metadata": "{}",
         "name": "seller_key",
         "type": "\"integer\""
        },
        {
         "metadata": "{}",
         "name": "date_key",
         "type": "\"integer\""
        },
        {
         "metadata": "{}",
         "name": "price",
         "type": "\"decimal(10,2)\""
        },
        {
         "metadata": "{}",
         "name": "freight_value",
         "type": "\"decimal(10,2)\""
        },
        {
         "metadata": "{}",
         "name": "total_amount",
         "type": "\"decimal(10,2)\""
        },
        {
         "metadata": "{}",
         "name": "payment_value",
         "type": "\"decimal(10,2)\""
        },
        {
         "metadata": "{}",
         "name": "payment_installments",
         "type": "\"integer\""
        },
        {
         "metadata": "{}",
         "name": "review_score",
         "type": "\"integer\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, sum as _sum, avg, coalesce, lit, to_date\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"\uD83C\uDF1F CREATING FACT_SALES (The Heart of Star Schema!)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Step 1: Start with ORDER ITEMS (this is our transaction grain - one row per item sold)\n",
    "print(\"\\n\uD83D\uDCE6 Step 1: Starting with ORDER ITEMS as base...\")\n",
    "fact_sales = df_order_items\n",
    "\n",
    "# Step 2: Join with ORDERS to get order details and customer_id\n",
    "print(\"\uD83D\uDCE6 Step 2: Joining with ORDERS...\")\n",
    "fact_sales = fact_sales.join(\n",
    "    df_orders,\n",
    "    on=\"order_id\",\n",
    "    how=\"inner\"\n",
    ")\n",
    "\n",
    "# Step 3: Join with ORDER PAYMENTS to get payment info\n",
    "print(\"\uD83D\uDCE6 Step 3: Joining with ORDER PAYMENTS...\")\n",
    "# Note: One order can have multiple payments, so we'll aggregate by order_id first\n",
    "order_payments_agg = df_order_payments.groupBy(\"order_id\").agg(\n",
    "    _sum(\"payment_value\").alias(\"payment_value\"),\n",
    "    avg(\"payment_installments\").alias(\"payment_installments\")\n",
    ")\n",
    "\n",
    "fact_sales = fact_sales.join(\n",
    "    order_payments_agg,\n",
    "    on=\"order_id\",\n",
    "    how=\"left\"  # Left join because not all orders may have payment data\n",
    ")\n",
    "\n",
    "# Step 4: Join with ORDER REVIEWS to get review scores\n",
    "print(\"\uD83D\uDCE6 Step 4: Joining with ORDER REVIEWS...\")\n",
    "# Keep only review_score, as one order can have only one review\n",
    "order_reviews_simple = df_order_reviews.select(\"order_id\", \"review_score\")\n",
    "\n",
    "fact_sales = fact_sales.join(\n",
    "    order_reviews_simple,\n",
    "    on=\"order_id\",\n",
    "    how=\"left\"  # Left join because not all orders have reviews\n",
    ")\n",
    "\n",
    "# Step 5: Join with DIM_CUSTOMER to get customer_key\n",
    "print(\"\uD83D\uDCE6 Step 5: Joining with DIM_CUSTOMER to get customer_key...\")\n",
    "fact_sales = fact_sales.join(\n",
    "    dim_customer.select(\"customer_id\", \"customer_key\"),\n",
    "    on=\"customer_id\",\n",
    "    how=\"inner\"\n",
    ")\n",
    "\n",
    "# Step 6: Join with DIM_PRODUCT to get product_key\n",
    "print(\"\uD83D\uDCE6 Step 6: Joining with DIM_PRODUCT to get product_key...\")\n",
    "fact_sales = fact_sales.join(\n",
    "    dim_product.select(\"product_id\", \"product_key\"),\n",
    "    on=\"product_id\",\n",
    "    how=\"inner\"\n",
    ")\n",
    "\n",
    "# Step 7: Join with DIM_SELLER to get seller_key\n",
    "print(\"\uD83D\uDCE6 Step 7: Joining with DIM_SELLER to get seller_key...\")\n",
    "fact_sales = fact_sales.join(\n",
    "    dim_seller.select(\"seller_id\", \"seller_key\"),\n",
    "    on=\"seller_id\",\n",
    "    how=\"inner\"\n",
    ")\n",
    "\n",
    "# Step 8: Join with DIM_DATE to get date_key\n",
    "print(\"\uD83D\uDCE6 Step 8: Joining with DIM_DATE to get date_key...\")\n",
    "# Convert order_purchase_timestamp to date and create date_key\n",
    "fact_sales = fact_sales.withColumn(\n",
    "    \"order_date\", \n",
    "    to_date(col(\"order_purchase_timestamp\"))\n",
    ")\n",
    "\n",
    "fact_sales = fact_sales.withColumn(\n",
    "    \"date_key\",\n",
    "    date_format(col(\"order_date\"), \"yyyyMMdd\").cast(\"int\")\n",
    ")\n",
    "\n",
    "# Verify date_key exists in dim_date\n",
    "fact_sales = fact_sales.join(\n",
    "    dim_date.select(\"date_key\"),\n",
    "    on=\"date_key\",\n",
    "    how=\"inner\"\n",
    ")\n",
    "\n",
    "# Step 9: Create surrogate key for FACT_SALES\n",
    "print(\"\uD83D\uDCE6 Step 9: Creating sale_key (surrogate key)...\")\n",
    "fact_sales = fact_sales.withColumn(\n",
    "    \"sale_key\",\n",
    "    row_number().over(Window.orderBy(\"order_id\", \"order_item_id\"))\n",
    ")\n",
    "\n",
    "# Step 10: Calculate total_amount (price + freight)\n",
    "print(\"\uD83D\uDCE6 Step 10: Calculating total_amount...\")\n",
    "fact_sales = fact_sales.withColumn(\n",
    "    \"total_amount\",\n",
    "    col(\"price\") + col(\"freight_value\")\n",
    ")\n",
    "\n",
    "# Step 11: Select final columns for FACT_SALES\n",
    "print(\"\uD83D\uDCE6 Step 11: Selecting final columns...\")\n",
    "fact_sales = fact_sales.select(\n",
    "    col(\"sale_key\"),                    # Primary key\n",
    "    col(\"order_id\"),                    # Original order ID (for reference)\n",
    "    col(\"order_item_id\"),               # Item sequence in order\n",
    "    \n",
    "    # Foreign Keys (links to dimension tables)\n",
    "    col(\"customer_key\"),                # → DIM_CUSTOMER\n",
    "    col(\"product_key\"),                 # → DIM_PRODUCT\n",
    "    col(\"seller_key\"),                  # → DIM_SELLER\n",
    "    col(\"date_key\"),                    # → DIM_DATE\n",
    "    \n",
    "    # Measures (the numbers we analyze)\n",
    "    col(\"price\").cast(\"decimal(10,2)\"),                      # Product price\n",
    "    col(\"freight_value\").cast(\"decimal(10,2)\"),              # Shipping cost\n",
    "    col(\"total_amount\").cast(\"decimal(10,2)\"),               # Total (price + freight)\n",
    "    coalesce(col(\"payment_value\"), lit(0)).cast(\"decimal(10,2)\").alias(\"payment_value\"),  # Payment amount\n",
    "    coalesce(col(\"payment_installments\"), lit(1)).cast(\"int\").alias(\"payment_installments\"),  # Number of installments\n",
    "    coalesce(col(\"review_score\"), lit(0)).cast(\"int\").alias(\"review_score\")  # Customer rating (1-5)\n",
    ")\n",
    "\n",
    "print(f\"\\n✅ FACT_SALES created with {fact_sales.count():,} rows\")\n",
    "print(\"\\n\uD83D\uDD0D Sample data:\")\n",
    "display(fact_sales.limit(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0889c873-27d6-490c-8a75-0ccb8e1b1e30",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n\uD83E\uDDEA VALIDATING FACT_SALES\n============================================================\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/databricks/python/lib/python3.12/site-packages/pyspark/sql/connect/expressions.py:1134: UserWarning: WARN WindowExpression: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n  warnings.warn(\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n✅ Total Sales Records: 113,314\n\n\uD83D\uDD0D Checking for NULL keys (should be 0):\n+------------+-----------+----------+--------+\n|customer_key|product_key|seller_key|date_key|\n+------------+-----------+----------+--------+\n|           0|          0|         0|       0|\n+------------+-----------+----------+--------+\n\n\n\uD83D\uDCCA Sales Metrics Summary:\n+-------------+---------------+---------------------+---------------------+------------------+\n|total_revenue|avg_order_value|total_product_revenue|total_freight_revenue|  avg_review_score|\n+-------------+---------------+---------------------+---------------------+------------------+\n|  15915872.32|     140.458128|          13651923.47|           2263948.85|3.9989498208517924|\n+-------------+---------------+---------------------+---------------------+------------------+\n\n\n\uD83D\uDCCA Sales by Year:\n+----+----------+------------+\n|year|   revenue|transactions|\n+----+----------+------------+\n|2016|  57364.03|         374|\n|2017|7185617.25|       51269|\n|2018|8672891.04|       61671|\n+----+----------+------------+\n\n\n\uD83D\uDCCA Top 5 Customers by Revenue:\n+------------+-----------+\n|customer_key|total_spent|\n+------------+-----------+\n|        8547|   13664.08|\n|       91986|    7274.88|\n|       77523|    6929.31|\n|       95125|    6922.21|\n|       24772|    6726.66|\n+------------+-----------+\n\n\n\uD83D\uDCCA Top 10 Product Categories by Revenue:\n+----------------------+----------+----------+\n|product_category_name |revenue   |items_sold|\n+----------------------+----------+----------+\n|beleza_saude          |1446622.08|9727      |\n|relogios_presentes    |1306761.40|6001      |\n|cama_mesa_banho       |1258189.51|11270     |\n|esporte_lazer         |1163329.98|8700      |\n|informatica_acessorios|1068070.48|7894      |\n|moveis_decoracao      |910683.05 |8415      |\n|utilidades_domesticas |781170.03 |6989      |\n|cool_stuff            |721492.90 |3806      |\n|automotivo            |687374.19 |4256      |\n|ferramentas_jardim    |585646.54 |4361      |\n+----------------------+----------+----------+\n\n\n\uD83D\uDCCA Average Review Score by Customer Segment:\n+----------------+-----------------+------+\n|customer_segment|       avg_rating|orders|\n+----------------+-----------------+------+\n|          Tier-2|3.893205018312164| 25666|\n|          Tier-1|4.058749010292953| 25260|\n|           Metro|4.018240687311662| 62388|\n+----------------+-----------------+------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import count, when\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"\uD83E\uDDEA VALIDATING FACT_SALES\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Check 1: Row count\n",
    "print(f\"\\n✅ Total Sales Records: {fact_sales.count():,}\")\n",
    "\n",
    "# Check 2: Check for NULL keys (should be 0)\n",
    "print(\"\\n\uD83D\uDD0D Checking for NULL keys (should be 0):\")\n",
    "fact_sales.select([\n",
    "    _sum(when(col(c).isNull(), 1).otherwise(0)).alias(c) \n",
    "    for c in [\"customer_key\", \"product_key\", \"seller_key\", \"date_key\"]\n",
    "]).show()\n",
    "\n",
    "# Check 3: Summary statistics\n",
    "print(\"\\n\uD83D\uDCCA Sales Metrics Summary:\")\n",
    "fact_sales.select(\n",
    "    _sum(\"total_amount\").alias(\"total_revenue\"),\n",
    "    avg(\"total_amount\").alias(\"avg_order_value\"),\n",
    "    _sum(\"price\").alias(\"total_product_revenue\"),\n",
    "    _sum(\"freight_value\").alias(\"total_freight_revenue\"),\n",
    "    avg(\"review_score\").alias(\"avg_review_score\")\n",
    ").show()\n",
    "\n",
    "# Check 4: Distribution by year\n",
    "print(\"\\n\uD83D\uDCCA Sales by Year:\")\n",
    "fact_sales.join(dim_date.select(\"date_key\", \"year\"), on=\"date_key\") \\\n",
    "    .groupBy(\"year\") \\\n",
    "    .agg(\n",
    "        _sum(\"total_amount\").alias(\"revenue\"),\n",
    "        count(\"*\").alias(\"transactions\")  # Now count is imported!\n",
    "    ) \\\n",
    "    .orderBy(\"year\") \\\n",
    "    .show()\n",
    "\n",
    "# Check 5: Top 5 customers by revenue\n",
    "print(\"\\n\uD83D\uDCCA Top 5 Customers by Revenue:\")\n",
    "fact_sales.groupBy(\"customer_key\") \\\n",
    "    .agg(_sum(\"total_amount\").alias(\"total_spent\")) \\\n",
    "    .orderBy(col(\"total_spent\").desc()) \\\n",
    "    .limit(5) \\\n",
    "    .show()\n",
    "\n",
    "# Check 6: Sales distribution by product category\n",
    "print(\"\\n\uD83D\uDCCA Top 10 Product Categories by Revenue:\")\n",
    "fact_sales.join(dim_product.select(\"product_key\", \"product_category_name\"), on=\"product_key\") \\\n",
    "    .groupBy(\"product_category_name\") \\\n",
    "    .agg(\n",
    "        _sum(\"total_amount\").alias(\"revenue\"),\n",
    "        count(\"*\").alias(\"items_sold\")\n",
    "    ) \\\n",
    "    .orderBy(col(\"revenue\").desc()) \\\n",
    "    .limit(10) \\\n",
    "    .show(truncate=False)\n",
    "\n",
    "# Check 7: Average review score by customer segment\n",
    "print(\"\\n\uD83D\uDCCA Average Review Score by Customer Segment:\")\n",
    "fact_sales.join(dim_customer.select(\"customer_key\", \"customer_segment\"), on=\"customer_key\") \\\n",
    "    .groupBy(\"customer_segment\") \\\n",
    "    .agg(\n",
    "        avg(\"review_score\").alias(\"avg_rating\"),\n",
    "        count(\"*\").alias(\"orders\")\n",
    "    ) \\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "238e2fd5-6b51-4f4a-860a-987e50d955de",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n\uD83C\uDF1F STAR SCHEMA COMPLETE! \uD83C\uDF1F\n================================================================================\n\n\uD83D\uDCCA DIMENSION TABLES:\n  ✅ DIM_DATE:          1,461 rows\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/databricks/python/lib/python3.12/site-packages/pyspark/sql/connect/expressions.py:1134: UserWarning: WARN WindowExpression: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n  warnings.warn(\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ✅ DIM_CUSTOMER:     99,441 rows\n  ✅ DIM_PRODUCT:      32,951 rows\n  ✅ DIM_SELLER:        3,095 rows\n\n⭐ FACT TABLE:\n  ✅ FACT_SALES:      113,314 rows\n\n================================================================================\n\uD83C\uDF89 Your E-Commerce Data Warehouse is Ready for Analytics!\n================================================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"\uD83C\uDF1F STAR SCHEMA COMPLETE! \uD83C\uDF1F\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(\"\\n\uD83D\uDCCA DIMENSION TABLES:\")\n",
    "print(f\"  ✅ DIM_DATE:     {dim_date.count():>10,} rows\")\n",
    "print(f\"  ✅ DIM_CUSTOMER: {dim_customer.count():>10,} rows\")\n",
    "print(f\"  ✅ DIM_PRODUCT:  {dim_product.count():>10,} rows\")\n",
    "print(f\"  ✅ DIM_SELLER:   {dim_seller.count():>10,} rows\")\n",
    "\n",
    "print(\"\\n⭐ FACT TABLE:\")\n",
    "print(f\"  ✅ FACT_SALES:   {fact_sales.count():>10,} rows\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"\uD83C\uDF89 Your E-Commerce Data Warehouse is Ready for Analytics!\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "37594b83-2000-4a6b-8774-6669f42e25d5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n\uD83D\uDCDD REGISTERING TABLES FOR SQL QUERIES\n============================================================\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/databricks/python/lib/python3.12/site-packages/pyspark/sql/connect/expressions.py:1134: UserWarning: WARN WindowExpression: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n  warnings.warn(\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ All tables registered!\nYou can now use SQL with: fact_sales, dim_customer, dim_product, dim_seller, dim_date\n"
     ]
    }
   ],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"\uD83D\uDCDD REGISTERING TABLES FOR SQL QUERIES\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Register all tables as temporary views\n",
    "fact_sales.createOrReplaceTempView(\"fact_sales\")\n",
    "dim_customer.createOrReplaceTempView(\"dim_customer\")\n",
    "dim_product.createOrReplaceTempView(\"dim_product\")\n",
    "dim_seller.createOrReplaceTempView(\"dim_seller\")\n",
    "dim_date.createOrReplaceTempView(\"dim_date\")\n",
    "\n",
    "print(\"✅ All tables registered!\")\n",
    "print(\"You can now use SQL with: fact_sales, dim_customer, dim_product, dim_seller, dim_date\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "67109263-d50c-4c78-95cf-17ad6f3eb733",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Query 1: Monthly Sales Trends \uD83D\uDCC8\n",
    "Business Question: \"What are our sales by month? Are we growing?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "570b1e83-6d42-4644-adf1-2c4a4319d3cb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>year</th><th>month</th><th>month_name</th><th>total_orders</th><th>total_items</th><th>revenue</th><th>avg_order_value</th></tr></thead><tbody><tr><td>2016</td><td>9</td><td>September</td><td>3</td><td>6</td><td>354.75</td><td>59.13</td></tr><tr><td>2016</td><td>10</td><td>October</td><td>308</td><td>367</td><td>56989.66</td><td>155.29</td></tr><tr><td>2016</td><td>12</td><td>December</td><td>1</td><td>1</td><td>19.62</td><td>19.62</td></tr><tr><td>2017</td><td>1</td><td>January</td><td>789</td><td>966</td><td>138160.22</td><td>143.02</td></tr><tr><td>2017</td><td>2</td><td>February</td><td>1733</td><td>1962</td><td>287698.56</td><td>146.64</td></tr><tr><td>2017</td><td>3</td><td>March</td><td>2641</td><td>3020</td><td>434044.94</td><td>143.72</td></tr><tr><td>2017</td><td>4</td><td>April</td><td>2391</td><td>2693</td><td>413387.27</td><td>153.50</td></tr><tr><td>2017</td><td>5</td><td>May</td><td>3660</td><td>4182</td><td>590516.91</td><td>141.20</td></tr><tr><td>2017</td><td>6</td><td>June</td><td>3217</td><td>3619</td><td>507123.25</td><td>140.13</td></tr><tr><td>2017</td><td>7</td><td>July</td><td>3969</td><td>4565</td><td>588966.63</td><td>129.02</td></tr><tr><td>2017</td><td>8</td><td>August</td><td>4293</td><td>4964</td><td>673795.98</td><td>135.74</td></tr><tr><td>2017</td><td>9</td><td>September</td><td>4243</td><td>4865</td><td>723299.96</td><td>148.67</td></tr><tr><td>2017</td><td>10</td><td>October</td><td>4568</td><td>5369</td><td>774005.04</td><td>144.16</td></tr><tr><td>2017</td><td>11</td><td>November</td><td>7451</td><td>8729</td><td>1187779.95</td><td>136.07</td></tr><tr><td>2017</td><td>12</td><td>December</td><td>5624</td><td>6335</td><td>866838.54</td><td>136.83</td></tr><tr><td>2018</td><td>1</td><td>January</td><td>7220</td><td>8263</td><td>1113929.01</td><td>134.81</td></tr><tr><td>2018</td><td>2</td><td>February</td><td>6694</td><td>7772</td><td>998137.75</td><td>128.43</td></tr><tr><td>2018</td><td>3</td><td>March</td><td>7188</td><td>8265</td><td>1159663.98</td><td>140.31</td></tr><tr><td>2018</td><td>4</td><td>April</td><td>6934</td><td>7986</td><td>1162227.22</td><td>145.53</td></tr><tr><td>2018</td><td>5</td><td>May</td><td>6853</td><td>7935</td><td>1150474.33</td><td>144.99</td></tr><tr><td>2018</td><td>6</td><td>June</td><td>6160</td><td>7084</td><td>1023674.47</td><td>144.51</td></tr><tr><td>2018</td><td>7</td><td>July</td><td>6273</td><td>7115</td><td>1061204.65</td><td>149.15</td></tr><tr><td>2018</td><td>8</td><td>August</td><td>6452</td><td>7250</td><td>1003413.17</td><td>138.40</td></tr><tr><td>2018</td><td>9</td><td>September</td><td>1</td><td>1</td><td>166.46</td><td>166.46</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         2016,
         9,
         "September",
         3,
         6,
         "354.75",
         "59.13"
        ],
        [
         2016,
         10,
         "October",
         308,
         367,
         "56989.66",
         "155.29"
        ],
        [
         2016,
         12,
         "December",
         1,
         1,
         "19.62",
         "19.62"
        ],
        [
         2017,
         1,
         "January",
         789,
         966,
         "138160.22",
         "143.02"
        ],
        [
         2017,
         2,
         "February",
         1733,
         1962,
         "287698.56",
         "146.64"
        ],
        [
         2017,
         3,
         "March",
         2641,
         3020,
         "434044.94",
         "143.72"
        ],
        [
         2017,
         4,
         "April",
         2391,
         2693,
         "413387.27",
         "153.50"
        ],
        [
         2017,
         5,
         "May",
         3660,
         4182,
         "590516.91",
         "141.20"
        ],
        [
         2017,
         6,
         "June",
         3217,
         3619,
         "507123.25",
         "140.13"
        ],
        [
         2017,
         7,
         "July",
         3969,
         4565,
         "588966.63",
         "129.02"
        ],
        [
         2017,
         8,
         "August",
         4293,
         4964,
         "673795.98",
         "135.74"
        ],
        [
         2017,
         9,
         "September",
         4243,
         4865,
         "723299.96",
         "148.67"
        ],
        [
         2017,
         10,
         "October",
         4568,
         5369,
         "774005.04",
         "144.16"
        ],
        [
         2017,
         11,
         "November",
         7451,
         8729,
         "1187779.95",
         "136.07"
        ],
        [
         2017,
         12,
         "December",
         5624,
         6335,
         "866838.54",
         "136.83"
        ],
        [
         2018,
         1,
         "January",
         7220,
         8263,
         "1113929.01",
         "134.81"
        ],
        [
         2018,
         2,
         "February",
         6694,
         7772,
         "998137.75",
         "128.43"
        ],
        [
         2018,
         3,
         "March",
         7188,
         8265,
         "1159663.98",
         "140.31"
        ],
        [
         2018,
         4,
         "April",
         6934,
         7986,
         "1162227.22",
         "145.53"
        ],
        [
         2018,
         5,
         "May",
         6853,
         7935,
         "1150474.33",
         "144.99"
        ],
        [
         2018,
         6,
         "June",
         6160,
         7084,
         "1023674.47",
         "144.51"
        ],
        [
         2018,
         7,
         "July",
         6273,
         7115,
         "1061204.65",
         "149.15"
        ],
        [
         2018,
         8,
         "August",
         6452,
         7250,
         "1003413.17",
         "138.40"
        ],
        [
         2018,
         9,
         "September",
         1,
         1,
         "166.46",
         "166.46"
        ]
       ],
       "datasetInfos": [
        {
         "name": "_sqldf",
         "schema": {
          "fields": [
           {
            "metadata": {},
            "name": "year",
            "nullable": true,
            "type": "integer"
           },
           {
            "metadata": {},
            "name": "month",
            "nullable": true,
            "type": "integer"
           },
           {
            "metadata": {},
            "name": "month_name",
            "nullable": true,
            "type": "string"
           },
           {
            "metadata": {},
            "name": "total_orders",
            "nullable": false,
            "type": "long"
           },
           {
            "metadata": {},
            "name": "total_items",
            "nullable": false,
            "type": "long"
           },
           {
            "metadata": {},
            "name": "revenue",
            "nullable": true,
            "type": "decimal(21,2)"
           },
           {
            "metadata": {},
            "name": "avg_order_value",
            "nullable": true,
            "type": "decimal(11,2)"
           }
          ],
          "type": "struct"
         },
         "tableIdentifier": null,
         "typeStr": "pyspark.sql.connect.dataframe.DataFrame"
        }
       ],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {
        "createTempViewForImplicitDf": true,
        "dataframeName": "_sqldf",
        "executionCount": 70
       },
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "year",
         "type": "\"integer\""
        },
        {
         "metadata": "{}",
         "name": "month",
         "type": "\"integer\""
        },
        {
         "metadata": "{}",
         "name": "month_name",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "total_orders",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "total_items",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "revenue",
         "type": "\"decimal(21,2)\""
        },
        {
         "metadata": "{}",
         "name": "avg_order_value",
         "type": "\"decimal(11,2)\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%sql\n",
    "SELECT \n",
    "    d.year,\n",
    "    d.month,\n",
    "    d.month_name,\n",
    "    COUNT(DISTINCT f.order_id) AS total_orders,\n",
    "    COUNT(*) AS total_items,\n",
    "    ROUND(SUM(f.total_amount), 2) AS revenue,\n",
    "    ROUND(AVG(f.total_amount), 2) AS avg_order_value\n",
    "FROM fact_sales f\n",
    "JOIN dim_date d ON f.date_key = d.date_key\n",
    "GROUP BY d.year, d.month, d.month_name\n",
    "ORDER BY d.year, d.month"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6d7377d1-8dc3-4e95-b475-1999694737f9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Query 2: Top 10 Best-Selling Products \uD83C\uDFC6\n",
    "Business Question: \"Which products generate the most revenue?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "090fe1dc-75f4-4dc0-86cf-f225819cb4a8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>category</th><th>units_sold</th><th>revenue</th><th>avg_price</th><th>avg_rating</th></tr></thead><tbody><tr><td>beleza_saude</td><td>9727</td><td>1446622.08</td><td>148.72</td><td>4.11</td></tr><tr><td>relogios_presentes</td><td>6001</td><td>1306761.40</td><td>217.76</td><td>3.99</td></tr><tr><td>cama_mesa_banho</td><td>11270</td><td>1258189.51</td><td>111.64</td><td>3.85</td></tr><tr><td>esporte_lazer</td><td>8700</td><td>1163329.98</td><td>133.72</td><td>4.08</td></tr><tr><td>informatica_acessorios</td><td>7894</td><td>1068070.48</td><td>135.30</td><td>3.91</td></tr><tr><td>moveis_decoracao</td><td>8415</td><td>910683.05</td><td>108.22</td><td>3.86</td></tr><tr><td>utilidades_domesticas</td><td>6989</td><td>781170.03</td><td>111.77</td><td>4.03</td></tr><tr><td>cool_stuff</td><td>3806</td><td>721492.90</td><td>189.57</td><td>4.11</td></tr><tr><td>automotivo</td><td>4256</td><td>687374.19</td><td>161.51</td><td>4.02</td></tr><tr><td>ferramentas_jardim</td><td>4361</td><td>585646.54</td><td>134.29</td><td>4.01</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "beleza_saude",
         9727,
         "1446622.08",
         "148.72",
         4.11
        ],
        [
         "relogios_presentes",
         6001,
         "1306761.40",
         "217.76",
         3.99
        ],
        [
         "cama_mesa_banho",
         11270,
         "1258189.51",
         "111.64",
         3.85
        ],
        [
         "esporte_lazer",
         8700,
         "1163329.98",
         "133.72",
         4.08
        ],
        [
         "informatica_acessorios",
         7894,
         "1068070.48",
         "135.30",
         3.91
        ],
        [
         "moveis_decoracao",
         8415,
         "910683.05",
         "108.22",
         3.86
        ],
        [
         "utilidades_domesticas",
         6989,
         "781170.03",
         "111.77",
         4.03
        ],
        [
         "cool_stuff",
         3806,
         "721492.90",
         "189.57",
         4.11
        ],
        [
         "automotivo",
         4256,
         "687374.19",
         "161.51",
         4.02
        ],
        [
         "ferramentas_jardim",
         4361,
         "585646.54",
         "134.29",
         4.01
        ]
       ],
       "datasetInfos": [
        {
         "name": "_sqldf",
         "schema": {
          "fields": [
           {
            "metadata": {},
            "name": "category",
            "nullable": true,
            "type": "string"
           },
           {
            "metadata": {},
            "name": "units_sold",
            "nullable": false,
            "type": "long"
           },
           {
            "metadata": {},
            "name": "revenue",
            "nullable": true,
            "type": "decimal(21,2)"
           },
           {
            "metadata": {},
            "name": "avg_price",
            "nullable": true,
            "type": "decimal(11,2)"
           },
           {
            "metadata": {},
            "name": "avg_rating",
            "nullable": true,
            "type": "double"
           }
          ],
          "type": "struct"
         },
         "tableIdentifier": null,
         "typeStr": "pyspark.sql.connect.dataframe.DataFrame"
        }
       ],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {
        "createTempViewForImplicitDf": true,
        "dataframeName": "_sqldf",
        "executionCount": 85
       },
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "category",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "units_sold",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "revenue",
         "type": "\"decimal(21,2)\""
        },
        {
         "metadata": "{}",
         "name": "avg_price",
         "type": "\"decimal(11,2)\""
        },
        {
         "metadata": "{}",
         "name": "avg_rating",
         "type": "\"double\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%sql\n",
    "SELECT \n",
    "    p.product_category_name AS category,\n",
    "    COUNT(*) AS units_sold,\n",
    "    ROUND(SUM(f.total_amount), 2) AS revenue,\n",
    "    ROUND(AVG(f.total_amount), 2) AS avg_price,\n",
    "    ROUND(AVG(f.review_score), 2) AS avg_rating\n",
    "FROM fact_sales f\n",
    "JOIN dim_product p ON f.product_key = p.product_key\n",
    "WHERE p.product_category_name != 'Unknown'\n",
    "GROUP BY p.product_category_name\n",
    "ORDER BY revenue DESC\n",
    "LIMIT 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "447dda7a-bb9d-4642-b687-fc540fa25585",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Query 3: Top 10 Customers by Spending \uD83D\uDC51\n",
    "Business Question: \"Who are our VIP customers?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0abd4051-19d4-44a2-b579-7290404b47ed",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>customer_key</th><th>customer_city</th><th>customer_state</th><th>customer_segment</th><th>total_orders</th><th>total_items</th><th>total_spent</th><th>avg_order_value</th><th>avg_rating</th></tr></thead><tbody><tr><td>8547</td><td>rio de janeiro</td><td>RJ</td><td>Metro</td><td>1</td><td>8</td><td>13664.08</td><td>1708.01</td><td>1.0</td></tr><tr><td>91986</td><td>vila velha</td><td>ES</td><td>Tier-2</td><td>1</td><td>4</td><td>7274.88</td><td>1818.72</td><td>1.0</td></tr><tr><td>77523</td><td>campo grande</td><td>MS</td><td>Tier-2</td><td>1</td><td>1</td><td>6929.31</td><td>6929.31</td><td>5.0</td></tr><tr><td>95125</td><td>vitoria</td><td>ES</td><td>Tier-2</td><td>1</td><td>1</td><td>6922.21</td><td>6922.21</td><td>0.0</td></tr><tr><td>24772</td><td>marilia</td><td>SP</td><td>Metro</td><td>1</td><td>1</td><td>6726.66</td><td>6726.66</td><td>5.0</td></tr><tr><td>2066</td><td>divinopolis</td><td>MG</td><td>Tier-1</td><td>1</td><td>6</td><td>6081.54</td><td>1013.59</td><td>1.0</td></tr><tr><td>86909</td><td>araruama</td><td>RJ</td><td>Metro</td><td>1</td><td>1</td><td>4950.34</td><td>4950.34</td><td>5.0</td></tr><tr><td>87398</td><td>goiania</td><td>GO</td><td>Tier-2</td><td>1</td><td>2</td><td>4809.44</td><td>2404.72</td><td>1.0</td></tr><tr><td>14283</td><td>maua</td><td>SP</td><td>Metro</td><td>1</td><td>1</td><td>4764.34</td><td>4764.34</td><td>4.0</td></tr><tr><td>23933</td><td>joao pessoa</td><td>PB</td><td>Tier-2</td><td>1</td><td>1</td><td>4681.78</td><td>4681.78</td><td>5.0</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         8547,
         "rio de janeiro",
         "RJ",
         "Metro",
         1,
         8,
         "13664.08",
         "1708.01",
         1.0
        ],
        [
         91986,
         "vila velha",
         "ES",
         "Tier-2",
         1,
         4,
         "7274.88",
         "1818.72",
         1.0
        ],
        [
         77523,
         "campo grande",
         "MS",
         "Tier-2",
         1,
         1,
         "6929.31",
         "6929.31",
         5.0
        ],
        [
         95125,
         "vitoria",
         "ES",
         "Tier-2",
         1,
         1,
         "6922.21",
         "6922.21",
         0.0
        ],
        [
         24772,
         "marilia",
         "SP",
         "Metro",
         1,
         1,
         "6726.66",
         "6726.66",
         5.0
        ],
        [
         2066,
         "divinopolis",
         "MG",
         "Tier-1",
         1,
         6,
         "6081.54",
         "1013.59",
         1.0
        ],
        [
         86909,
         "araruama",
         "RJ",
         "Metro",
         1,
         1,
         "4950.34",
         "4950.34",
         5.0
        ],
        [
         87398,
         "goiania",
         "GO",
         "Tier-2",
         1,
         2,
         "4809.44",
         "2404.72",
         1.0
        ],
        [
         14283,
         "maua",
         "SP",
         "Metro",
         1,
         1,
         "4764.34",
         "4764.34",
         4.0
        ],
        [
         23933,
         "joao pessoa",
         "PB",
         "Tier-2",
         1,
         1,
         "4681.78",
         "4681.78",
         5.0
        ]
       ],
       "datasetInfos": [
        {
         "name": "_sqldf",
         "schema": {
          "fields": [
           {
            "metadata": {},
            "name": "customer_key",
            "nullable": false,
            "type": "integer"
           },
           {
            "metadata": {},
            "name": "customer_city",
            "nullable": true,
            "type": "string"
           },
           {
            "metadata": {},
            "name": "customer_state",
            "nullable": true,
            "type": "string"
           },
           {
            "metadata": {},
            "name": "customer_segment",
            "nullable": false,
            "type": "string"
           },
           {
            "metadata": {},
            "name": "total_orders",
            "nullable": false,
            "type": "long"
           },
           {
            "metadata": {},
            "name": "total_items",
            "nullable": false,
            "type": "long"
           },
           {
            "metadata": {},
            "name": "total_spent",
            "nullable": true,
            "type": "decimal(21,2)"
           },
           {
            "metadata": {},
            "name": "avg_order_value",
            "nullable": true,
            "type": "decimal(11,2)"
           },
           {
            "metadata": {},
            "name": "avg_rating",
            "nullable": true,
            "type": "double"
           }
          ],
          "type": "struct"
         },
         "tableIdentifier": null,
         "typeStr": "pyspark.sql.connect.dataframe.DataFrame"
        }
       ],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {
        "createTempViewForImplicitDf": true,
        "dataframeName": "_sqldf",
        "executionCount": 86
       },
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "customer_key",
         "type": "\"integer\""
        },
        {
         "metadata": "{}",
         "name": "customer_city",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "customer_state",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "customer_segment",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "total_orders",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "total_items",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "total_spent",
         "type": "\"decimal(21,2)\""
        },
        {
         "metadata": "{}",
         "name": "avg_order_value",
         "type": "\"decimal(11,2)\""
        },
        {
         "metadata": "{}",
         "name": "avg_rating",
         "type": "\"double\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%sql\n",
    "SELECT \n",
    "    c.customer_key,\n",
    "    c.customer_city,\n",
    "    c.customer_state,\n",
    "    c.customer_segment,\n",
    "    COUNT(DISTINCT f.order_id) AS total_orders,\n",
    "    COUNT(*) AS total_items,\n",
    "    ROUND(SUM(f.total_amount), 2) AS total_spent,\n",
    "    ROUND(AVG(f.total_amount), 2) AS avg_order_value,\n",
    "    ROUND(AVG(f.review_score), 2) AS avg_rating\n",
    "FROM fact_sales f\n",
    "JOIN dim_customer c ON f.customer_key = c.customer_key\n",
    "GROUP BY c.customer_key, c.customer_city, c.customer_state, c.customer_segment\n",
    "ORDER BY total_spent DESC\n",
    "LIMIT 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6cde59d7-ac83-4f86-bad1-b333e89a92c9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Query 4: Top Performing Sellers \uD83C\uDFEA\n",
    "Business Question: \"Which sellers are our star performers?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ce1f44af-5e96-481a-be2b-3fd291a701ec",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>seller_city</th><th>seller_state</th><th>total_orders</th><th>total_items_sold</th><th>revenue</th><th>avg_rating</th></tr></thead><tbody><tr><td>sao paulo</td><td>SP</td><td>24588</td><td>28100</td><td>3193209.58</td><td>3.99</td></tr><tr><td>ibitinga</td><td>SP</td><td>6565</td><td>7878</td><td>774236.48</td><td>3.75</td></tr><tr><td>curitiba</td><td>PR</td><td>2654</td><td>2967</td><td>541210.83</td><td>4.21</td></tr><tr><td>rio de janeiro</td><td>RJ</td><td>2188</td><td>2440</td><td>404447.63</td><td>4.01</td></tr><tr><td>guarulhos</td><td>SP</td><td>2071</td><td>2380</td><td>378781.90</td><td>4.05</td></tr><tr><td>ribeirao preto</td><td>SP</td><td>2019</td><td>2279</td><td>316404.30</td><td>3.83</td></tr><tr><td>itaquaquecetuba</td><td>SP</td><td>1241</td><td>1664</td><td>292783.61</td><td>3.37</td></tr><tr><td>santo andre</td><td>SP</td><td>2711</td><td>3016</td><td>278369.03</td><td>4.07</td></tr><tr><td>guariba</td><td>SP</td><td>1132</td><td>1156</td><td>249640.70</td><td>4.09</td></tr><tr><td>maringa</td><td>PR</td><td>1860</td><td>2252</td><td>249425.73</td><td>3.92</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "sao paulo",
         "SP",
         24588,
         28100,
         "3193209.58",
         3.99
        ],
        [
         "ibitinga",
         "SP",
         6565,
         7878,
         "774236.48",
         3.75
        ],
        [
         "curitiba",
         "PR",
         2654,
         2967,
         "541210.83",
         4.21
        ],
        [
         "rio de janeiro",
         "RJ",
         2188,
         2440,
         "404447.63",
         4.01
        ],
        [
         "guarulhos",
         "SP",
         2071,
         2380,
         "378781.90",
         4.05
        ],
        [
         "ribeirao preto",
         "SP",
         2019,
         2279,
         "316404.30",
         3.83
        ],
        [
         "itaquaquecetuba",
         "SP",
         1241,
         1664,
         "292783.61",
         3.37
        ],
        [
         "santo andre",
         "SP",
         2711,
         3016,
         "278369.03",
         4.07
        ],
        [
         "guariba",
         "SP",
         1132,
         1156,
         "249640.70",
         4.09
        ],
        [
         "maringa",
         "PR",
         1860,
         2252,
         "249425.73",
         3.92
        ]
       ],
       "datasetInfos": [
        {
         "name": "_sqldf",
         "schema": {
          "fields": [
           {
            "metadata": {},
            "name": "seller_city",
            "nullable": true,
            "type": "string"
           },
           {
            "metadata": {},
            "name": "seller_state",
            "nullable": true,
            "type": "string"
           },
           {
            "metadata": {},
            "name": "total_orders",
            "nullable": false,
            "type": "long"
           },
           {
            "metadata": {},
            "name": "total_items_sold",
            "nullable": false,
            "type": "long"
           },
           {
            "metadata": {},
            "name": "revenue",
            "nullable": true,
            "type": "decimal(21,2)"
           },
           {
            "metadata": {},
            "name": "avg_rating",
            "nullable": true,
            "type": "double"
           }
          ],
          "type": "struct"
         },
         "tableIdentifier": null,
         "typeStr": "pyspark.sql.connect.dataframe.DataFrame"
        }
       ],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {
        "createTempViewForImplicitDf": true,
        "dataframeName": "_sqldf",
        "executionCount": 91
       },
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "seller_city",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "seller_state",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "total_orders",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "total_items_sold",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "revenue",
         "type": "\"decimal(21,2)\""
        },
        {
         "metadata": "{}",
         "name": "avg_rating",
         "type": "\"double\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%sql\n",
    "SELECT \n",
    "    s.seller_city,\n",
    "    s.seller_state,\n",
    "    COUNT(DISTINCT f.order_id) AS total_orders,\n",
    "    COUNT(*) AS total_items_sold,\n",
    "    ROUND(SUM(f.total_amount), 2) AS revenue,\n",
    "    ROUND(AVG(f.review_score), 2) AS avg_rating\n",
    "FROM fact_sales f\n",
    "JOIN dim_seller s ON f.seller_key = s.seller_key\n",
    "GROUP BY s.seller_city, s.seller_state\n",
    "ORDER BY revenue DESC\n",
    "LIMIT 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f5ca40f9-f428-499f-bd7c-de395abef95c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Query 5: Revenue by State (Customers) \n",
    "\uD83D\uDCCDBusiness Question: \"Which states are our biggest markets?\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "256b9637-9a79-441f-8797-291f65bec36a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>state</th><th>segment</th><th>total_customers</th><th>total_orders</th><th>revenue</th><th>avg_order_value</th></tr></thead><tbody><tr><td>SP</td><td>Metro</td><td>41375</td><td>41375</td><td>5951489.17</td><td>124.72</td></tr><tr><td>RJ</td><td>Metro</td><td>12762</td><td>12762</td><td>2139076.28</td><td>145.83</td></tr><tr><td>MG</td><td>Tier-1</td><td>11544</td><td>11544</td><td>1863794.44</td><td>141.10</td></tr><tr><td>RS</td><td>Tier-1</td><td>5432</td><td>5432</td><td>890763.29</td><td>141.68</td></tr><tr><td>PR</td><td>Tier-1</td><td>4998</td><td>4998</td><td>804321.70</td><td>139.54</td></tr><tr><td>BA</td><td>Tier-2</td><td>3358</td><td>3358</td><td>613777.23</td><td>160.72</td></tr><tr><td>SC</td><td>Tier-2</td><td>3612</td><td>3612</td><td>612153.65</td><td>146.06</td></tr><tr><td>DF</td><td>Tier-2</td><td>2125</td><td>2125</td><td>355792.69</td><td>146.24</td></tr><tr><td>GO</td><td>Tier-2</td><td>2007</td><td>2007</td><td>351160.09</td><td>149.24</td></tr><tr><td>ES</td><td>Tier-2</td><td>2025</td><td>2025</td><td>325949.95</td><td>143.65</td></tr><tr><td>PE</td><td>Tier-2</td><td>1648</td><td>1648</td><td>323523.14</td><td>177.96</td></tr><tr><td>CE</td><td>Tier-2</td><td>1327</td><td>1327</td><td>276437.11</td><td>186.40</td></tr><tr><td>PA</td><td>Tier-2</td><td>970</td><td>970</td><td>218282.36</td><td>200.81</td></tr><tr><td>MT</td><td>Tier-2</td><td>903</td><td>903</td><td>186523.57</td><td>176.30</td></tr><tr><td>MA</td><td>Tier-2</td><td>740</td><td>740</td><td>151510.13</td><td>182.76</td></tr><tr><td>PB</td><td>Tier-2</td><td>532</td><td>532</td><td>141102.54</td><td>234.00</td></tr><tr><td>MS</td><td>Tier-2</td><td>709</td><td>709</td><td>137595.08</td><td>165.58</td></tr><tr><td>PI</td><td>Tier-2</td><td>493</td><td>493</td><td>108213.50</td><td>199.29</td></tr><tr><td>RN</td><td>Tier-2</td><td>482</td><td>482</td><td>102013.56</td><td>192.12</td></tr><tr><td>AL</td><td>Tier-2</td><td>411</td><td>411</td><td>96532.09</td><td>215.47</td></tr><tr><td>SE</td><td>Tier-2</td><td>345</td><td>345</td><td>73032.32</td><td>189.69</td></tr><tr><td>TO</td><td>Tier-2</td><td>279</td><td>279</td><td>61354.42</td><td>194.78</td></tr><tr><td>RO</td><td>Tier-2</td><td>247</td><td>247</td><td>57558.02</td><td>207.04</td></tr><tr><td>AM</td><td>Tier-2</td><td>147</td><td>147</td><td>27918.87</td><td>168.19</td></tr><tr><td>AC</td><td>Tier-2</td><td>81</td><td>81</td><td>19669.70</td><td>213.80</td></tr><tr><td>AP</td><td>Tier-2</td><td>68</td><td>68</td><td>16262.80</td><td>198.33</td></tr><tr><td>RR</td><td>Tier-2</td><td>46</td><td>46</td><td>10064.62</td><td>193.55</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "SP",
         "Metro",
         41375,
         41375,
         "5951489.17",
         "124.72"
        ],
        [
         "RJ",
         "Metro",
         12762,
         12762,
         "2139076.28",
         "145.83"
        ],
        [
         "MG",
         "Tier-1",
         11544,
         11544,
         "1863794.44",
         "141.10"
        ],
        [
         "RS",
         "Tier-1",
         5432,
         5432,
         "890763.29",
         "141.68"
        ],
        [
         "PR",
         "Tier-1",
         4998,
         4998,
         "804321.70",
         "139.54"
        ],
        [
         "BA",
         "Tier-2",
         3358,
         3358,
         "613777.23",
         "160.72"
        ],
        [
         "SC",
         "Tier-2",
         3612,
         3612,
         "612153.65",
         "146.06"
        ],
        [
         "DF",
         "Tier-2",
         2125,
         2125,
         "355792.69",
         "146.24"
        ],
        [
         "GO",
         "Tier-2",
         2007,
         2007,
         "351160.09",
         "149.24"
        ],
        [
         "ES",
         "Tier-2",
         2025,
         2025,
         "325949.95",
         "143.65"
        ],
        [
         "PE",
         "Tier-2",
         1648,
         1648,
         "323523.14",
         "177.96"
        ],
        [
         "CE",
         "Tier-2",
         1327,
         1327,
         "276437.11",
         "186.40"
        ],
        [
         "PA",
         "Tier-2",
         970,
         970,
         "218282.36",
         "200.81"
        ],
        [
         "MT",
         "Tier-2",
         903,
         903,
         "186523.57",
         "176.30"
        ],
        [
         "MA",
         "Tier-2",
         740,
         740,
         "151510.13",
         "182.76"
        ],
        [
         "PB",
         "Tier-2",
         532,
         532,
         "141102.54",
         "234.00"
        ],
        [
         "MS",
         "Tier-2",
         709,
         709,
         "137595.08",
         "165.58"
        ],
        [
         "PI",
         "Tier-2",
         493,
         493,
         "108213.50",
         "199.29"
        ],
        [
         "RN",
         "Tier-2",
         482,
         482,
         "102013.56",
         "192.12"
        ],
        [
         "AL",
         "Tier-2",
         411,
         411,
         "96532.09",
         "215.47"
        ],
        [
         "SE",
         "Tier-2",
         345,
         345,
         "73032.32",
         "189.69"
        ],
        [
         "TO",
         "Tier-2",
         279,
         279,
         "61354.42",
         "194.78"
        ],
        [
         "RO",
         "Tier-2",
         247,
         247,
         "57558.02",
         "207.04"
        ],
        [
         "AM",
         "Tier-2",
         147,
         147,
         "27918.87",
         "168.19"
        ],
        [
         "AC",
         "Tier-2",
         81,
         81,
         "19669.70",
         "213.80"
        ],
        [
         "AP",
         "Tier-2",
         68,
         68,
         "16262.80",
         "198.33"
        ],
        [
         "RR",
         "Tier-2",
         46,
         46,
         "10064.62",
         "193.55"
        ]
       ],
       "datasetInfos": [
        {
         "name": "_sqldf",
         "schema": {
          "fields": [
           {
            "metadata": {},
            "name": "state",
            "nullable": true,
            "type": "string"
           },
           {
            "metadata": {},
            "name": "segment",
            "nullable": false,
            "type": "string"
           },
           {
            "metadata": {},
            "name": "total_customers",
            "nullable": false,
            "type": "long"
           },
           {
            "metadata": {},
            "name": "total_orders",
            "nullable": false,
            "type": "long"
           },
           {
            "metadata": {},
            "name": "revenue",
            "nullable": true,
            "type": "decimal(21,2)"
           },
           {
            "metadata": {},
            "name": "avg_order_value",
            "nullable": true,
            "type": "decimal(11,2)"
           }
          ],
          "type": "struct"
         },
         "tableIdentifier": null,
         "typeStr": "pyspark.sql.connect.dataframe.DataFrame"
        }
       ],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {
        "createTempViewForImplicitDf": true,
        "dataframeName": "_sqldf",
        "executionCount": 92
       },
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "state",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "segment",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "total_customers",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "total_orders",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "revenue",
         "type": "\"decimal(21,2)\""
        },
        {
         "metadata": "{}",
         "name": "avg_order_value",
         "type": "\"decimal(11,2)\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%sql\n",
    "SELECT \n",
    "    c.customer_state AS state,\n",
    "    c.customer_segment AS segment,\n",
    "    COUNT(DISTINCT c.customer_key) AS total_customers,\n",
    "    COUNT(DISTINCT f.order_id) AS total_orders,\n",
    "    ROUND(SUM(f.total_amount), 2) AS revenue,\n",
    "    ROUND(AVG(f.total_amount), 2) AS avg_order_value\n",
    "FROM fact_sales f\n",
    "JOIN dim_customer c ON f.customer_key = c.customer_key\n",
    "GROUP BY c.customer_state, c.customer_segment\n",
    "ORDER BY revenue DESC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "209d00a6-74c3-47e8-98e0-f48da6aefc75",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ SUCCESS! You CAN use Volumes for Delta Lake!\n"
     ]
    }
   ],
   "source": [
    "# Test if you can write to Volumes\n",
    "try:\n",
    "    test_df = spark.createDataFrame([(1, \"test\")], [\"id\", \"value\"])\n",
    "    test_df.write.format(\"delta\").mode(\"overwrite\").save(\"/Volumes/sales_analysis/data/data/test_delta\")\n",
    "    print(\"✅ SUCCESS! You CAN use Volumes for Delta Lake!\")\n",
    "    dbutils.fs.rm(\"/Volumes/sales_analysis/data/data/test_delta\", recurse=True)  # Cleanup\n",
    "except Exception as e:\n",
    "    print(f\"❌ Cannot use Volumes: {e}\")\n",
    "    print(\"Use /FileStore/ instead\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8b1bebb5-83b2-466d-87a6-d66b8d698bdd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\uD83D\uDCC1 Using storage path: /Volumes/sales_analysis/data/delta_tables/\n"
     ]
    }
   ],
   "source": [
    "# Define storage paths\n",
    "USE_VOLUMES = True  # We'll test this first\n",
    "\n",
    "if USE_VOLUMES:\n",
    "    BASE_PATH = \"/Volumes/sales_analysis/data/delta_tables/\"\n",
    "else:\n",
    "    BASE_PATH = \"/FileStore/delta_tables/\"\n",
    "\n",
    "print(f\"\uD83D\uDCC1 Using storage path: {BASE_PATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d28c1525-d2c0-47ca-a515-0d78cb58309b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n\uD83D\uDCBE SAVING ALL TABLES TO DELTA LAKE\n============================================================\n\uD83D\uDCC1 Storage location: /Volumes/sales_analysis/data/data/\n\n\uD83D\uDCCA Saving Dimension Tables...\n  \uD83D\uDDD3️  Saving DIM_DATE...\n     ✅ Saved 1,461 rows\n  \uD83D\uDC65 Saving DIM_CUSTOMER...\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/databricks/python/lib/python3.12/site-packages/pyspark/sql/connect/expressions.py:1134: UserWarning: WARN WindowExpression: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n  warnings.warn(\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     ✅ Saved 99,441 rows\n  \uD83C\uDFF7️  Saving DIM_PRODUCT...\n     ✅ Saved 32,951 rows\n  \uD83C\uDFEA Saving DIM_SELLER...\n     ✅ Saved 3,095 rows\n\n⭐ Saving Fact Table...\n  \uD83C\uDF1F Saving FACT_SALES...\n     ✅ Saved 113,314 rows\n\n============================================================\n\uD83C\uDF89 ALL TABLES SAVED SUCCESSFULLY!\n============================================================\n\uD83D\uDCCD Location: /Volumes/sales_analysis/data/data/\n\n\uD83D\uDCA1 Your tables are now persistent and will survive cluster restarts!\n"
     ]
    }
   ],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"\uD83D\uDCBE SAVING ALL TABLES TO DELTA LAKE\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Use your existing volume path where CSVs are stored\n",
    "BASE_PATH = \"/Volumes/sales_analysis/data/data/\"\n",
    "print(f\"\uD83D\uDCC1 Storage location: {BASE_PATH}\\n\")\n",
    "\n",
    "# Save all dimension tables\n",
    "print(\"\uD83D\uDCCA Saving Dimension Tables...\")\n",
    "\n",
    "print(\"  \uD83D\uDDD3️  Saving DIM_DATE...\")\n",
    "dim_date.write.format(\"delta\").mode(\"overwrite\").save(BASE_PATH + \"dim_date_delta\")\n",
    "print(f\"     ✅ Saved {dim_date.count():,} rows\")\n",
    "\n",
    "print(\"  \uD83D\uDC65 Saving DIM_CUSTOMER...\")\n",
    "dim_customer.write.format(\"delta\").mode(\"overwrite\").save(BASE_PATH + \"dim_customer_delta\")\n",
    "print(f\"     ✅ Saved {dim_customer.count():,} rows\")\n",
    "\n",
    "print(\"  \uD83C\uDFF7️  Saving DIM_PRODUCT...\")\n",
    "dim_product.write.format(\"delta\").mode(\"overwrite\").save(BASE_PATH + \"dim_product_delta\")\n",
    "print(f\"     ✅ Saved {dim_product.count():,} rows\")\n",
    "\n",
    "print(\"  \uD83C\uDFEA Saving DIM_SELLER...\")\n",
    "dim_seller.write.format(\"delta\").mode(\"overwrite\").save(BASE_PATH + \"dim_seller_delta\")\n",
    "print(f\"     ✅ Saved {dim_seller.count():,} rows\")\n",
    "\n",
    "# Save fact table\n",
    "print(\"\\n⭐ Saving Fact Table...\")\n",
    "print(\"  \uD83C\uDF1F Saving FACT_SALES...\")\n",
    "fact_sales.write.format(\"delta\").mode(\"overwrite\").save(BASE_PATH + \"fact_sales_delta\")\n",
    "print(f\"     ✅ Saved {fact_sales.count():,} rows\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"\uD83C\uDF89 ALL TABLES SAVED SUCCESSFULLY!\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\uD83D\uDCCD Location: {BASE_PATH}\")\n",
    "print(\"\\n\uD83D\uDCA1 Your tables are now persistent and will survive cluster restarts!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "89f3834b-c2cc-4df4-9d4e-5cf7285307f0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n\uD83D\uDD0D VERIFYING SAVED DELTA TABLES\n============================================================\n\n\uD83D\uDCC2 Tables in Delta storage:\n  ✅ dim_customer_delta/\n  ✅ dim_date_delta/\n  ✅ dim_product_delta/\n  ✅ dim_seller_delta/\n  ✅ fact_sales_delta/\n  ✅ olist_customers_dataset.csv\n  ✅ olist_geolocation_dataset.csv\n  ✅ olist_order_items_dataset.csv\n  ✅ olist_order_payments_dataset.csv\n  ✅ olist_order_reviews_dataset.csv\n  ✅ olist_orders_dataset.csv\n  ✅ olist_products_dataset.csv\n  ✅ olist_sellers_dataset.csv\n  ✅ product_category_name_translation.csv\n\n\uD83D\uDCCA Table Details:\n------------------------------------------------------------\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n",
       "\u001B[0;31mAnalysisException\u001B[0m                         Traceback (most recent call last)\n",
       "File \u001B[0;32m<command-5195277928817664>, line 18\u001B[0m\n",
       "\u001B[1;32m     16\u001B[0m table_path \u001B[38;5;241m=\u001B[39m BASE_PATH \u001B[38;5;241m+\u001B[39m table_name\n",
       "\u001B[1;32m     17\u001B[0m df \u001B[38;5;241m=\u001B[39m spark\u001B[38;5;241m.\u001B[39mread\u001B[38;5;241m.\u001B[39mformat(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mdelta\u001B[39m\u001B[38;5;124m\"\u001B[39m)\u001B[38;5;241m.\u001B[39mload(table_path)\n",
       "\u001B[0;32m---> 18\u001B[0m row_count \u001B[38;5;241m=\u001B[39m df\u001B[38;5;241m.\u001B[39mcount()\n",
       "\u001B[1;32m     19\u001B[0m size_bytes \u001B[38;5;241m=\u001B[39m \u001B[38;5;28msum\u001B[39m([f\u001B[38;5;241m.\u001B[39msize \u001B[38;5;28;01mfor\u001B[39;00m f \u001B[38;5;129;01min\u001B[39;00m dbutils\u001B[38;5;241m.\u001B[39mfs\u001B[38;5;241m.\u001B[39mls(table_path) \u001B[38;5;28;01mif\u001B[39;00m f\u001B[38;5;241m.\u001B[39mname\u001B[38;5;241m.\u001B[39mendswith(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m.parquet\u001B[39m\u001B[38;5;124m'\u001B[39m)])\n",
       "\u001B[1;32m     20\u001B[0m size_mb \u001B[38;5;241m=\u001B[39m size_bytes \u001B[38;5;241m/\u001B[39m (\u001B[38;5;241m1024\u001B[39m \u001B[38;5;241m*\u001B[39m \u001B[38;5;241m1024\u001B[39m)\n",
       "\n",
       "File \u001B[0;32m/databricks/python/lib/python3.12/site-packages/pyspark/sql/connect/dataframe.py:317\u001B[0m, in \u001B[0;36mDataFrame.count\u001B[0;34m(self)\u001B[0m\n",
       "\u001B[1;32m    314\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mcount\u001B[39m(\u001B[38;5;28mself\u001B[39m) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m \u001B[38;5;28mint\u001B[39m:\n",
       "\u001B[1;32m    315\u001B[0m     table, _ \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39magg(\n",
       "\u001B[1;32m    316\u001B[0m         F\u001B[38;5;241m.\u001B[39m_invoke_function(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcount\u001B[39m\u001B[38;5;124m\"\u001B[39m, F\u001B[38;5;241m.\u001B[39mlit(\u001B[38;5;241m1\u001B[39m))\n",
       "\u001B[0;32m--> 317\u001B[0m     )\u001B[38;5;241m.\u001B[39m_to_table()  \u001B[38;5;66;03m# type: ignore[operator]\u001B[39;00m\n",
       "\u001B[1;32m    318\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m table[\u001B[38;5;241m0\u001B[39m][\u001B[38;5;241m0\u001B[39m]\u001B[38;5;241m.\u001B[39mas_py()\n",
       "\n",
       "File \u001B[0;32m/databricks/python/lib/python3.12/site-packages/pyspark/sql/connect/dataframe.py:1890\u001B[0m, in \u001B[0;36mDataFrame._to_table\u001B[0;34m(self)\u001B[0m\n",
       "\u001B[1;32m   1888\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_to_table\u001B[39m(\u001B[38;5;28mself\u001B[39m) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Tuple[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mpa.Table\u001B[39m\u001B[38;5;124m\"\u001B[39m, Optional[StructType]]:\n",
       "\u001B[1;32m   1889\u001B[0m     query \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_plan\u001B[38;5;241m.\u001B[39mto_proto(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_session\u001B[38;5;241m.\u001B[39mclient)\n",
       "\u001B[0;32m-> 1890\u001B[0m     table, schema, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_execution_info \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_session\u001B[38;5;241m.\u001B[39mclient\u001B[38;5;241m.\u001B[39mto_table(\n",
       "\u001B[1;32m   1891\u001B[0m         query, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_plan\u001B[38;5;241m.\u001B[39mobservations\n",
       "\u001B[1;32m   1892\u001B[0m     )\n",
       "\u001B[1;32m   1893\u001B[0m     \u001B[38;5;28;01massert\u001B[39;00m table \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n",
       "\u001B[1;32m   1894\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m (table, schema)\n",
       "\n",
       "File \u001B[0;32m/databricks/python/lib/python3.12/site-packages/pyspark/sql/connect/client/core.py:1239\u001B[0m, in \u001B[0;36mSparkConnectClient.to_table\u001B[0;34m(self, plan, observations)\u001B[0m\n",
       "\u001B[1;32m   1237\u001B[0m req \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_execute_plan_request_with_metadata()\n",
       "\u001B[1;32m   1238\u001B[0m req\u001B[38;5;241m.\u001B[39mplan\u001B[38;5;241m.\u001B[39mCopyFrom(plan)\n",
       "\u001B[0;32m-> 1239\u001B[0m table, schema, metrics, observed_metrics, _ \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_execute_and_fetch(req, observations)\n",
       "\u001B[1;32m   1241\u001B[0m \u001B[38;5;66;03m# Create a query execution object.\u001B[39;00m\n",
       "\u001B[1;32m   1242\u001B[0m ei \u001B[38;5;241m=\u001B[39m ExecutionInfo(metrics, observed_metrics)\n",
       "\n",
       "File \u001B[0;32m/databricks/python/lib/python3.12/site-packages/pyspark/sql/connect/client/core.py:2059\u001B[0m, in \u001B[0;36mSparkConnectClient._execute_and_fetch\u001B[0;34m(self, req, observations, extra_request_metadata, self_destruct)\u001B[0m\n",
       "\u001B[1;32m   2056\u001B[0m properties: Dict[\u001B[38;5;28mstr\u001B[39m, Any] \u001B[38;5;241m=\u001B[39m {}\n",
       "\u001B[1;32m   2058\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m Progress(handlers\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_progress_handlers, operation_id\u001B[38;5;241m=\u001B[39mreq\u001B[38;5;241m.\u001B[39moperation_id) \u001B[38;5;28;01mas\u001B[39;00m progress:\n",
       "\u001B[0;32m-> 2059\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m response \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_execute_and_fetch_as_iterator(\n",
       "\u001B[1;32m   2060\u001B[0m         req, observations, extra_request_metadata \u001B[38;5;129;01mor\u001B[39;00m [], progress\u001B[38;5;241m=\u001B[39mprogress\n",
       "\u001B[1;32m   2061\u001B[0m     ):\n",
       "\u001B[1;32m   2062\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(response, StructType):\n",
       "\u001B[1;32m   2063\u001B[0m             schema \u001B[38;5;241m=\u001B[39m response\n",
       "\n",
       "File \u001B[0;32m/databricks/python/lib/python3.12/site-packages/pyspark/sql/connect/client/core.py:2035\u001B[0m, in \u001B[0;36mSparkConnectClient._execute_and_fetch_as_iterator\u001B[0;34m(self, req, observations, extra_request_metadata, progress)\u001B[0m\n",
       "\u001B[1;32m   2033\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m kb\n",
       "\u001B[1;32m   2034\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m error:\n",
       "\u001B[0;32m-> 2035\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_handle_error(error)\n",
       "\n",
       "File \u001B[0;32m/databricks/python/lib/python3.12/site-packages/pyspark/sql/connect/client/core.py:2355\u001B[0m, in \u001B[0;36mSparkConnectClient._handle_error\u001B[0;34m(self, error)\u001B[0m\n",
       "\u001B[1;32m   2353\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mthread_local\u001B[38;5;241m.\u001B[39minside_error_handling \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n",
       "\u001B[1;32m   2354\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(error, grpc\u001B[38;5;241m.\u001B[39mRpcError):\n",
       "\u001B[0;32m-> 2355\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_handle_rpc_error(error)\n",
       "\u001B[1;32m   2356\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m error\n",
       "\u001B[1;32m   2357\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n",
       "\n",
       "File \u001B[0;32m/databricks/python/lib/python3.12/site-packages/pyspark/sql/connect/client/core.py:2433\u001B[0m, in \u001B[0;36mSparkConnectClient._handle_rpc_error\u001B[0;34m(self, rpc_error)\u001B[0m\n",
       "\u001B[1;32m   2429\u001B[0m             logger\u001B[38;5;241m.\u001B[39mdebug(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mReceived ErrorInfo: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00minfo\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n",
       "\u001B[1;32m   2431\u001B[0m             \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_handle_rpc_error_with_error_info(info, status\u001B[38;5;241m.\u001B[39mmessage, status_code)  \u001B[38;5;66;03m# EDGE\u001B[39;00m\n",
       "\u001B[0;32m-> 2433\u001B[0m             \u001B[38;5;28;01mraise\u001B[39;00m convert_exception(\n",
       "\u001B[1;32m   2434\u001B[0m                 info,\n",
       "\u001B[1;32m   2435\u001B[0m                 status\u001B[38;5;241m.\u001B[39mmessage,\n",
       "\u001B[1;32m   2436\u001B[0m                 \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_fetch_enriched_error(info),\n",
       "\u001B[1;32m   2437\u001B[0m                 \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_display_server_stack_trace(),\n",
       "\u001B[1;32m   2438\u001B[0m                 status_code,\n",
       "\u001B[1;32m   2439\u001B[0m             ) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n",
       "\u001B[1;32m   2441\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m SparkConnectGrpcException(\n",
       "\u001B[1;32m   2442\u001B[0m         message\u001B[38;5;241m=\u001B[39mstatus\u001B[38;5;241m.\u001B[39mmessage,\n",
       "\u001B[1;32m   2443\u001B[0m         sql_state\u001B[38;5;241m=\u001B[39mErrorCode\u001B[38;5;241m.\u001B[39mCLIENT_UNEXPECTED_MISSING_SQL_STATE,  \u001B[38;5;66;03m# EDGE\u001B[39;00m\n",
       "\u001B[1;32m   2444\u001B[0m         grpc_status_code\u001B[38;5;241m=\u001B[39mstatus_code,\n",
       "\u001B[1;32m   2445\u001B[0m     ) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n",
       "\u001B[1;32m   2446\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n",
       "\n",
       "\u001B[0;31mAnalysisException\u001B[0m: [PATH_NOT_FOUND] Path does not exist: /Volumes/sales_analysis/data/data/dim_date. SQLSTATE: 42K03\n",
       "\n",
       "JVM stacktrace:\n",
       "org.apache.spark.sql.AnalysisException\n",
       "\tat org.apache.spark.sql.errors.QueryCompilationErrors$.dataPathNotExistError(QueryCompilationErrors.scala:2655)\n",
       "\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.$anonfun$toBaseRelation$1(DeltaTableV2.scala:596)\n",
       "\tat com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)\n",
       "\tat com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:200)\n",
       "\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)\n",
       "\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:532)\n",
       "\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:530)\n",
       "\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.recordFrameProfile(DeltaTableV2.scala:186)\n",
       "\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.toBaseRelation$lzycompute(DeltaTableV2.scala:589)\n",
       "\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.toBaseRelation(DeltaTableV2.scala:588)\n",
       "\tat com.databricks.sql.transaction.tahoe.sources.DeltaDataSource.$anonfun$createRelation$6(DeltaDataSource.scala:416)\n",
       "\tat com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)\n",
       "\tat com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:200)\n",
       "\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)\n",
       "\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:532)\n",
       "\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:530)\n",
       "\tat com.databricks.sql.transaction.tahoe.sources.DeltaDataSource.recordFrameProfile(DeltaDataSource.scala:101)\n",
       "\tat com.databricks.sql.transaction.tahoe.sources.DeltaDataSource.createRelation(DeltaDataSource.scala:379)\n",
       "\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:448)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource.org$apache$spark$sql$catalyst$analysis$ResolveDataSource$$loadV1BatchSource(ResolveDataSource.scala:310)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource$$anonfun$apply$1.$anonfun$applyOrElse$5(ResolveDataSource.scala:117)\n",
       "\tat scala.Option.getOrElse(Option.scala:201)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource$$anonfun$apply$1.applyOrElse(ResolveDataSource.scala:116)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource$$anonfun$apply$1.applyOrElse(ResolveDataSource.scala:64)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$3(AnalysisHelper.scala:141)\n",
       "\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:121)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:141)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:418)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:137)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:133)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:45)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp(AnalysisHelper.scala:114)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp$(AnalysisHelper.scala:113)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUp(LogicalPlan.scala:45)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource.apply(ResolveDataSource.scala:64)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource.apply(ResolveDataSource.scala:62)\n",
       "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$17(RuleExecutor.scala:510)\n",
       "\tat org.apache.spark.sql.catalyst.rules.RecoverableRuleExecutionHelper.processRule(RuleExecutor.scala:664)\n",
       "\tat org.apache.spark.sql.catalyst.rules.RecoverableRuleExecutionHelper.processRule$(RuleExecutor.scala:648)\n",
       "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.processRule(RuleExecutor.scala:144)\n",
       "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$16(RuleExecutor.scala:510)\n",
       "\tat com.databricks.spark.util.MemoryTracker$.withThreadAllocatedBytes(MemoryTracker.scala:51)\n",
       "\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.measureRule(QueryPlanningTracker.scala:350)\n",
       "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$15(RuleExecutor.scala:508)\n",
       "\tat com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)\n",
       "\tat com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:200)\n",
       "\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)\n",
       "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$14(RuleExecutor.scala:507)\n",
       "\tat scala.collection.LinearSeqOps.foldLeft(LinearSeq.scala:183)\n",
       "\tat scala.collection.LinearSeqOps.foldLeft$(LinearSeq.scala:179)\n",
       "\tat scala.collection.immutable.List.foldLeft(List.scala:79)\n",
       "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$13(RuleExecutor.scala:499)\n",
       "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n",
       "\tat com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)\n",
       "\tat com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:200)\n",
       "\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)\n",
       "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.executeBatch$1(RuleExecutor.scala:473)\n",
       "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$23(RuleExecutor.scala:620)\n",
       "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$23$adapted(RuleExecutor.scala:620)\n",
       "\tat scala.collection.immutable.List.foreach(List.scala:334)\n",
       "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:620)\n",
       "\tat com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)\n",
       "\tat com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:200)\n",
       "\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)\n",
       "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:366)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.super$execute(Analyzer.scala:710)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeSameContext$1(Analyzer.scala:710)\n",
       "\tat com.databricks.sql.unity.SAMSnapshotHelper$.visitPlansDuringAnalysis(SAMSnapshotHelper.scala:41)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeSameContext(Analyzer.scala:709)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:682)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:453)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:682)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:605)\n",
       "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:354)\n",
       "\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:265)\n",
       "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:354)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.resolveInFixedPoint(HybridAnalyzer.scala:437)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.$anonfun$apply$1(HybridAnalyzer.scala:98)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.withTrackedAnalyzerBridgeState(HybridAnalyzer.scala:135)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.apply(HybridAnalyzer.scala:91)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$2(Analyzer.scala:662)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:425)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:662)\n",
       "\tat com.databricks.sql.unity.SAMSnapshotHelper$.visitPlansDuringAnalysis(SAMSnapshotHelper.scala:41)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:654)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$3(QueryExecution.scala:356)\n",
       "\tat com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)\n",
       "\tat com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:200)\n",
       "\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)\n",
       "\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:698)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$8(QueryExecution.scala:873)\n",
       "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withExecutionPhase$1(SQLExecution.scala:161)\n",
       "\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:348)\n",
       "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)\n",
       "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:344)\n",
       "\tat com.databricks.util.TracingSpanUtils$.$anonfun$withTracing$4(TracingSpanUtils.scala:235)\n",
       "\tat com.databricks.util.TracingSpanUtils$.withTracing(TracingSpanUtils.scala:129)\n",
       "\tat com.databricks.util.TracingSpanUtils$.withTracing(TracingSpanUtils.scala:233)\n",
       "\tat com.databricks.tracing.TracingUtils$.withTracing(TracingUtils.scala:296)\n",
       "\tat com.databricks.spark.util.DatabricksTracingHelper.withSpan(DatabricksSparkTracingHelper.scala:112)\n",
       "\tat com.databricks.spark.util.DBRTracing$.withSpan(DBRTracing.scala:47)\n",
       "\tat org.apache.spark.sql.execution.SQLExecution$.withExecutionPhase(SQLExecution.scala:142)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$7(QueryExecution.scala:873)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:1524)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$5(QueryExecution.scala:866)\n",
       "\tat com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$4(QueryExecution.scala:863)\n",
       "\tat com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$3(QueryExecution.scala:863)\n",
       "\tat com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:862)\n",
       "\tat com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.withQueryExecutionId(QueryExecution.scala:850)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:861)\n",
       "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:860)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:860)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$2(QueryExecution.scala:338)\n",
       "\tat com.databricks.sql.util.MemoryTrackerHelper.withMemoryTracking(MemoryTrackerHelper.scala:111)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$1(QueryExecution.scala:337)\n",
       "\tat scala.util.Try$.apply(Try.scala:217)\n",
       "\tat org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1684)\n",
       "\tat org.apache.spark.util.LazyTry.tryT$lzycompute(LazyTry.scala:60)\n",
       "\tat org.apache.spark.util.LazyTry.tryT(LazyTry.scala:59)\n",
       "\tat org.apache.spark.util.LazyTry.get(LazyTry.scala:75)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:398)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:311)\n",
       "\tat org.apache.spark.sql.classic.Dataset$.$anonfun$ofRows$1(Dataset.scala:128)\n",
       "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:860)\n",
       "\tat org.apache.spark.sql.classic.SparkSession.$anonfun$withActiveAndFrameProfiler$1(SparkSession.scala:1116)\n",
       "\tat com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)\n",
       "\tat com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:200)\n",
       "\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)\n",
       "\tat org.apache.spark.sql.classic.SparkSession.withActiveAndFrameProfiler(SparkSession.scala:1116)\n",
       "\tat org.apache.spark.sql.classic.Dataset$.ofRows(Dataset.scala:126)\n",
       "\tat org.apache.spark.sql.classic.DataFrameReader.load(DataFrameReader.scala:170)\n",
       "\tat org.apache.spark.sql.classic.DataFrameReader.load(DataFrameReader.scala:148)\n",
       "\tat org.apache.spark.sql.connect.planner.SparkConnectPlanner.transformReadRel(SparkConnectPlanner.scala:2173)\n",
       "\tat org.apache.spark.sql.connect.planner.SparkConnectPlanner.$anonfun$transformRelation$1(SparkConnectPlanner.scala:216)\n",
       "\tat org.apache.spark.sql.connect.service.SessionHolder.$anonfun$usePlanCache$8(SessionHolder.scala:739)\n",
       "\tat org.apache.spark.sql.connect.service.SessionHolder.measureSubtreeRelationNodes(SessionHolder.scala:755)\n",
       "\tat org.apache.spark.sql.connect.service.SessionHolder.$anonfun$usePlanCache$6(SessionHolder.scala:738)\n",
       "\tat org.apache.spark.util.Utils$.getTryWithCallerStacktrace(Utils.scala:1745)\n",
       "\tat org.apache.spark.util.LazyTry.get(LazyTry.scala:75)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.executedPlan(QueryExecution.scala:785)\n",
       "\tat com.databricks.spark.sqlgateway.history.SqlExecutionMetrics.$anonfun$setQueryExecution$1(SqlExecutionMetrics.scala:190)\n",
       "\tat scala.Option.flatMap(Option.scala:283)\n",
       "\tat com.databricks.spark.sqlgateway.history.SqlExecutionMetrics.setQueryExecution(SqlExecutionMetrics.scala:190)\n",
       "\tat com.databricks.spark.sqlgateway.history.SqlGatewayHistorySparkListener.$anonfun$onSqlStart$1(SqlGatewayHistorySparkListener.scala:841)\n",
       "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n",
       "\tat com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)\n",
       "\tat com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:200)\n",
       "\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)\n",
       "\tat com.databricks.spark.sqlgateway.history.SqlGatewayHistorySparkListener.com$databricks$spark$sqlgateway$history$SqlGatewayHistorySparkListener$$onSqlStart(SqlGatewayHistorySparkListener.scala:789)\n",
       "\tat com.databricks.spark.sqlgateway.history.SqlGatewayHistorySparkListener$$anonfun$onOtherEventDefault$1.applyOrElse(SqlGatewayHistorySparkListener.scala:230)\n",
       "\tat com.databricks.spark.sqlgateway.history.SqlGatewayHistorySparkListener$$anonfun$onOtherEventDefault$1.applyOrElse(SqlGatewayHistorySparkListener.scala:218)\n",
       "\tat scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:35)\n",
       "\tat com.databricks.spark.sqlgateway.history.utils.ScriptStatementHelper$$anonfun$onOtherEvent$1.applyOrElse(ScriptStatementHelper.scala:28)\n",
       "\tat com.databricks.spark.sqlgateway.history.utils.ScriptStatementHelper$$anonfun$onOtherEvent$1.applyOrElse(ScriptStatementHelper.scala:28)\n",
       "\tat scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:270)\n",
       "\tat com.databricks.spark.sqlgateway.history.SqlGatewayHistorySparkListener.$anonfun$onOtherEvent$1(SqlGatewayHistorySparkListener.scala:196)\n",
       "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n",
       "\tat com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)\n",
       "\tat com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:200)\n",
       "\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)\n",
       "\tat com.databricks.spark.sqlgateway.history.SqlGatewayHistorySparkListener.onOtherEvent(SqlGatewayHistorySparkListener.scala:196)\n",
       "\tat org.apache.spark.scheduler.SparkListenerBus.doPostEvent(SparkListenerBus.scala:108)\n",
       "\tat org.apache.spark.scheduler.SparkListenerBus.doPostEvent$(SparkListenerBus.scala:28)\n",
       "\tat org.apache.spark.scheduler.AsyncEventQueue.doPostEvent(AsyncEventQueue.scala:46)\n",
       "\tat org.apache.spark.scheduler.AsyncEventQueue.doPostEvent(AsyncEventQueue.scala:46)\n",
       "\tat org.apache.spark.util.ListenerBus.postToAll(ListenerBus.scala:208)\n",
       "\tat org.apache.spark.util.ListenerBus.postToAll$(ListenerBus.scala:172)\n",
       "\tat org.apache.spark.scheduler.AsyncEventQueue.super$postToAll(AsyncEventQueue.scala:150)\n",
       "\tat org.apache.spark.scheduler.AsyncEventQueue.$anonfun$dispatch$1(AsyncEventQueue.scala:150)\n",
       "\tat scala.runtime.java8.JFunction0$mcJ$sp.apply(JFunction0$mcJ$sp.scala:17)\n",
       "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)\n",
       "\tat org.apache.spark.scheduler.AsyncEventQueue.org$apache$spark$scheduler$AsyncEventQueue$$dispatch(AsyncEventQueue.scala:119)\n",
       "\tat org.apache.spark.scheduler.AsyncEventQueue$$anon$2.$anonfun$run$1(AsyncEventQueue.scala:115)\n",
       "\tat org.apache.spark.util.Utils$.tryOrStopSparkContext(Utils.scala:1572)\n",
       "\tat org.apache.spark.scheduler.AsyncEventQueue$$anon$2.run(AsyncEventQueue.scala:115)"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "datasetInfos": [],
       "jupyterProps": {
        "ename": "AnalysisException",
        "evalue": "[PATH_NOT_FOUND] Path does not exist: /Volumes/sales_analysis/data/data/dim_date. SQLSTATE: 42K03\n\nJVM stacktrace:\norg.apache.spark.sql.AnalysisException\n\tat org.apache.spark.sql.errors.QueryCompilationErrors$.dataPathNotExistError(QueryCompilationErrors.scala:2655)\n\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.$anonfun$toBaseRelation$1(DeltaTableV2.scala:596)\n\tat com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)\n\tat com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:200)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)\n\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:532)\n\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:530)\n\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.recordFrameProfile(DeltaTableV2.scala:186)\n\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.toBaseRelation$lzycompute(DeltaTableV2.scala:589)\n\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.toBaseRelation(DeltaTableV2.scala:588)\n\tat com.databricks.sql.transaction.tahoe.sources.DeltaDataSource.$anonfun$createRelation$6(DeltaDataSource.scala:416)\n\tat com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)\n\tat com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:200)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)\n\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:532)\n\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:530)\n\tat com.databricks.sql.transaction.tahoe.sources.DeltaDataSource.recordFrameProfile(DeltaDataSource.scala:101)\n\tat com.databricks.sql.transaction.tahoe.sources.DeltaDataSource.createRelation(DeltaDataSource.scala:379)\n\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:448)\n\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource.org$apache$spark$sql$catalyst$analysis$ResolveDataSource$$loadV1BatchSource(ResolveDataSource.scala:310)\n\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource$$anonfun$apply$1.$anonfun$applyOrElse$5(ResolveDataSource.scala:117)\n\tat scala.Option.getOrElse(Option.scala:201)\n\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource$$anonfun$apply$1.applyOrElse(ResolveDataSource.scala:116)\n\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource$$anonfun$apply$1.applyOrElse(ResolveDataSource.scala:64)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$3(AnalysisHelper.scala:141)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:121)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:141)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:418)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:137)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:133)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:45)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp(AnalysisHelper.scala:114)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp$(AnalysisHelper.scala:113)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUp(LogicalPlan.scala:45)\n\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource.apply(ResolveDataSource.scala:64)\n\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource.apply(ResolveDataSource.scala:62)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$17(RuleExecutor.scala:510)\n\tat org.apache.spark.sql.catalyst.rules.RecoverableRuleExecutionHelper.processRule(RuleExecutor.scala:664)\n\tat org.apache.spark.sql.catalyst.rules.RecoverableRuleExecutionHelper.processRule$(RuleExecutor.scala:648)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.processRule(RuleExecutor.scala:144)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$16(RuleExecutor.scala:510)\n\tat com.databricks.spark.util.MemoryTracker$.withThreadAllocatedBytes(MemoryTracker.scala:51)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.measureRule(QueryPlanningTracker.scala:350)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$15(RuleExecutor.scala:508)\n\tat com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)\n\tat com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:200)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$14(RuleExecutor.scala:507)\n\tat scala.collection.LinearSeqOps.foldLeft(LinearSeq.scala:183)\n\tat scala.collection.LinearSeqOps.foldLeft$(LinearSeq.scala:179)\n\tat scala.collection.immutable.List.foldLeft(List.scala:79)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$13(RuleExecutor.scala:499)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n\tat com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)\n\tat com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:200)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.executeBatch$1(RuleExecutor.scala:473)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$23(RuleExecutor.scala:620)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$23$adapted(RuleExecutor.scala:620)\n\tat scala.collection.immutable.List.foreach(List.scala:334)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:620)\n\tat com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)\n\tat com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:200)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:366)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.super$execute(Analyzer.scala:710)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeSameContext$1(Analyzer.scala:710)\n\tat com.databricks.sql.unity.SAMSnapshotHelper$.visitPlansDuringAnalysis(SAMSnapshotHelper.scala:41)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeSameContext(Analyzer.scala:709)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:682)\n\tat org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:453)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:682)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:605)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:354)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:265)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:354)\n\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.resolveInFixedPoint(HybridAnalyzer.scala:437)\n\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.$anonfun$apply$1(HybridAnalyzer.scala:98)\n\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.withTrackedAnalyzerBridgeState(HybridAnalyzer.scala:135)\n\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.apply(HybridAnalyzer.scala:91)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$2(Analyzer.scala:662)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:425)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:662)\n\tat com.databricks.sql.unity.SAMSnapshotHelper$.visitPlansDuringAnalysis(SAMSnapshotHelper.scala:41)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:654)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$3(QueryExecution.scala:356)\n\tat com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)\n\tat com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:200)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:698)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$8(QueryExecution.scala:873)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withExecutionPhase$1(SQLExecution.scala:161)\n\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:348)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)\n\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:344)\n\tat com.databricks.util.TracingSpanUtils$.$anonfun$withTracing$4(TracingSpanUtils.scala:235)\n\tat com.databricks.util.TracingSpanUtils$.withTracing(TracingSpanUtils.scala:129)\n\tat com.databricks.util.TracingSpanUtils$.withTracing(TracingSpanUtils.scala:233)\n\tat com.databricks.tracing.TracingUtils$.withTracing(TracingUtils.scala:296)\n\tat com.databricks.spark.util.DatabricksTracingHelper.withSpan(DatabricksSparkTracingHelper.scala:112)\n\tat com.databricks.spark.util.DBRTracing$.withSpan(DBRTracing.scala:47)\n\tat org.apache.spark.sql.execution.SQLExecution$.withExecutionPhase(SQLExecution.scala:142)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$7(QueryExecution.scala:873)\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:1524)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$5(QueryExecution.scala:866)\n\tat com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$4(QueryExecution.scala:863)\n\tat com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$3(QueryExecution.scala:863)\n\tat com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:862)\n\tat com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)\n\tat org.apache.spark.sql.execution.QueryExecution.withQueryExecutionId(QueryExecution.scala:850)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:861)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:860)\n\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:860)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$2(QueryExecution.scala:338)\n\tat com.databricks.sql.util.MemoryTrackerHelper.withMemoryTracking(MemoryTrackerHelper.scala:111)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$1(QueryExecution.scala:337)\n\tat scala.util.Try$.apply(Try.scala:217)\n\tat org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1684)\n\tat org.apache.spark.util.LazyTry.tryT$lzycompute(LazyTry.scala:60)\n\tat org.apache.spark.util.LazyTry.tryT(LazyTry.scala:59)\n\tat org.apache.spark.util.LazyTry.get(LazyTry.scala:75)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:398)\n\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:311)\n\tat org.apache.spark.sql.classic.Dataset$.$anonfun$ofRows$1(Dataset.scala:128)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:860)\n\tat org.apache.spark.sql.classic.SparkSession.$anonfun$withActiveAndFrameProfiler$1(SparkSession.scala:1116)\n\tat com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)\n\tat com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:200)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)\n\tat org.apache.spark.sql.classic.SparkSession.withActiveAndFrameProfiler(SparkSession.scala:1116)\n\tat org.apache.spark.sql.classic.Dataset$.ofRows(Dataset.scala:126)\n\tat org.apache.spark.sql.classic.DataFrameReader.load(DataFrameReader.scala:170)\n\tat org.apache.spark.sql.classic.DataFrameReader.load(DataFrameReader.scala:148)\n\tat org.apache.spark.sql.connect.planner.SparkConnectPlanner.transformReadRel(SparkConnectPlanner.scala:2173)\n\tat org.apache.spark.sql.connect.planner.SparkConnectPlanner.$anonfun$transformRelation$1(SparkConnectPlanner.scala:216)\n\tat org.apache.spark.sql.connect.service.SessionHolder.$anonfun$usePlanCache$8(SessionHolder.scala:739)\n\tat org.apache.spark.sql.connect.service.SessionHolder.measureSubtreeRelationNodes(SessionHolder.scala:755)\n\tat org.apache.spark.sql.connect.service.SessionHolder.$anonfun$usePlanCache$6(SessionHolder.scala:738)\n\tat org.apache.spark.util.Utils$.getTryWithCallerStacktrace(Utils.scala:1745)\n\tat org.apache.spark.util.LazyTry.get(LazyTry.scala:75)\n\tat org.apache.spark.sql.execution.QueryExecution.executedPlan(QueryExecution.scala:785)\n\tat com.databricks.spark.sqlgateway.history.SqlExecutionMetrics.$anonfun$setQueryExecution$1(SqlExecutionMetrics.scala:190)\n\tat scala.Option.flatMap(Option.scala:283)\n\tat com.databricks.spark.sqlgateway.history.SqlExecutionMetrics.setQueryExecution(SqlExecutionMetrics.scala:190)\n\tat com.databricks.spark.sqlgateway.history.SqlGatewayHistorySparkListener.$anonfun$onSqlStart$1(SqlGatewayHistorySparkListener.scala:841)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n\tat com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)\n\tat com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:200)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)\n\tat com.databricks.spark.sqlgateway.history.SqlGatewayHistorySparkListener.com$databricks$spark$sqlgateway$history$SqlGatewayHistorySparkListener$$onSqlStart(SqlGatewayHistorySparkListener.scala:789)\n\tat com.databricks.spark.sqlgateway.history.SqlGatewayHistorySparkListener$$anonfun$onOtherEventDefault$1.applyOrElse(SqlGatewayHistorySparkListener.scala:230)\n\tat com.databricks.spark.sqlgateway.history.SqlGatewayHistorySparkListener$$anonfun$onOtherEventDefault$1.applyOrElse(SqlGatewayHistorySparkListener.scala:218)\n\tat scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:35)\n\tat com.databricks.spark.sqlgateway.history.utils.ScriptStatementHelper$$anonfun$onOtherEvent$1.applyOrElse(ScriptStatementHelper.scala:28)\n\tat com.databricks.spark.sqlgateway.history.utils.ScriptStatementHelper$$anonfun$onOtherEvent$1.applyOrElse(ScriptStatementHelper.scala:28)\n\tat scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:270)\n\tat com.databricks.spark.sqlgateway.history.SqlGatewayHistorySparkListener.$anonfun$onOtherEvent$1(SqlGatewayHistorySparkListener.scala:196)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n\tat com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)\n\tat com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:200)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)\n\tat com.databricks.spark.sqlgateway.history.SqlGatewayHistorySparkListener.onOtherEvent(SqlGatewayHistorySparkListener.scala:196)\n\tat org.apache.spark.scheduler.SparkListenerBus.doPostEvent(SparkListenerBus.scala:108)\n\tat org.apache.spark.scheduler.SparkListenerBus.doPostEvent$(SparkListenerBus.scala:28)\n\tat org.apache.spark.scheduler.AsyncEventQueue.doPostEvent(AsyncEventQueue.scala:46)\n\tat org.apache.spark.scheduler.AsyncEventQueue.doPostEvent(AsyncEventQueue.scala:46)\n\tat org.apache.spark.util.ListenerBus.postToAll(ListenerBus.scala:208)\n\tat org.apache.spark.util.ListenerBus.postToAll$(ListenerBus.scala:172)\n\tat org.apache.spark.scheduler.AsyncEventQueue.super$postToAll(AsyncEventQueue.scala:150)\n\tat org.apache.spark.scheduler.AsyncEventQueue.$anonfun$dispatch$1(AsyncEventQueue.scala:150)\n\tat scala.runtime.java8.JFunction0$mcJ$sp.apply(JFunction0$mcJ$sp.scala:17)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)\n\tat org.apache.spark.scheduler.AsyncEventQueue.org$apache$spark$scheduler$AsyncEventQueue$$dispatch(AsyncEventQueue.scala:119)\n\tat org.apache.spark.scheduler.AsyncEventQueue$$anon$2.$anonfun$run$1(AsyncEventQueue.scala:115)\n\tat org.apache.spark.util.Utils$.tryOrStopSparkContext(Utils.scala:1572)\n\tat org.apache.spark.scheduler.AsyncEventQueue$$anon$2.run(AsyncEventQueue.scala:115)"
       },
       "metadata": {
        "errorSummary": "[PATH_NOT_FOUND] Path does not exist: /Volumes/sales_analysis/data/data/dim_date. SQLSTATE: 42K03"
       },
       "removedWidgets": [],
       "sqlProps": {
        "breakingChangeInfo": null,
        "errorClass": "PATH_NOT_FOUND",
        "pysparkCallSite": "",
        "pysparkFragment": "",
        "pysparkSummary": "",
        "sqlState": "42K03",
        "stackTrace": "org.apache.spark.sql.AnalysisException\n\tat org.apache.spark.sql.errors.QueryCompilationErrors$.dataPathNotExistError(QueryCompilationErrors.scala:2655)\n\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.$anonfun$toBaseRelation$1(DeltaTableV2.scala:596)\n\tat com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)\n\tat com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:200)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)\n\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:532)\n\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:530)\n\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.recordFrameProfile(DeltaTableV2.scala:186)\n\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.toBaseRelation$lzycompute(DeltaTableV2.scala:589)\n\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.toBaseRelation(DeltaTableV2.scala:588)\n\tat com.databricks.sql.transaction.tahoe.sources.DeltaDataSource.$anonfun$createRelation$6(DeltaDataSource.scala:416)\n\tat com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)\n\tat com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:200)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)\n\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:532)\n\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:530)\n\tat com.databricks.sql.transaction.tahoe.sources.DeltaDataSource.recordFrameProfile(DeltaDataSource.scala:101)\n\tat com.databricks.sql.transaction.tahoe.sources.DeltaDataSource.createRelation(DeltaDataSource.scala:379)\n\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:448)\n\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource.org$apache$spark$sql$catalyst$analysis$ResolveDataSource$$loadV1BatchSource(ResolveDataSource.scala:310)\n\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource$$anonfun$apply$1.$anonfun$applyOrElse$5(ResolveDataSource.scala:117)\n\tat scala.Option.getOrElse(Option.scala:201)\n\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource$$anonfun$apply$1.applyOrElse(ResolveDataSource.scala:116)\n\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource$$anonfun$apply$1.applyOrElse(ResolveDataSource.scala:64)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$3(AnalysisHelper.scala:141)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:121)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:141)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:418)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:137)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:133)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:45)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp(AnalysisHelper.scala:114)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp$(AnalysisHelper.scala:113)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUp(LogicalPlan.scala:45)\n\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource.apply(ResolveDataSource.scala:64)\n\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource.apply(ResolveDataSource.scala:62)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$17(RuleExecutor.scala:510)\n\tat org.apache.spark.sql.catalyst.rules.RecoverableRuleExecutionHelper.processRule(RuleExecutor.scala:664)\n\tat org.apache.spark.sql.catalyst.rules.RecoverableRuleExecutionHelper.processRule$(RuleExecutor.scala:648)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.processRule(RuleExecutor.scala:144)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$16(RuleExecutor.scala:510)\n\tat com.databricks.spark.util.MemoryTracker$.withThreadAllocatedBytes(MemoryTracker.scala:51)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.measureRule(QueryPlanningTracker.scala:350)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$15(RuleExecutor.scala:508)\n\tat com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)\n\tat com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:200)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$14(RuleExecutor.scala:507)\n\tat scala.collection.LinearSeqOps.foldLeft(LinearSeq.scala:183)\n\tat scala.collection.LinearSeqOps.foldLeft$(LinearSeq.scala:179)\n\tat scala.collection.immutable.List.foldLeft(List.scala:79)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$13(RuleExecutor.scala:499)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n\tat com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)\n\tat com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:200)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.executeBatch$1(RuleExecutor.scala:473)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$23(RuleExecutor.scala:620)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$23$adapted(RuleExecutor.scala:620)\n\tat scala.collection.immutable.List.foreach(List.scala:334)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:620)\n\tat com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)\n\tat com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:200)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:366)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.super$execute(Analyzer.scala:710)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeSameContext$1(Analyzer.scala:710)\n\tat com.databricks.sql.unity.SAMSnapshotHelper$.visitPlansDuringAnalysis(SAMSnapshotHelper.scala:41)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeSameContext(Analyzer.scala:709)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:682)\n\tat org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:453)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:682)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:605)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:354)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:265)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:354)\n\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.resolveInFixedPoint(HybridAnalyzer.scala:437)\n\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.$anonfun$apply$1(HybridAnalyzer.scala:98)\n\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.withTrackedAnalyzerBridgeState(HybridAnalyzer.scala:135)\n\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.apply(HybridAnalyzer.scala:91)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$2(Analyzer.scala:662)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:425)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:662)\n\tat com.databricks.sql.unity.SAMSnapshotHelper$.visitPlansDuringAnalysis(SAMSnapshotHelper.scala:41)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:654)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$3(QueryExecution.scala:356)\n\tat com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)\n\tat com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:200)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:698)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$8(QueryExecution.scala:873)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withExecutionPhase$1(SQLExecution.scala:161)\n\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:348)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)\n\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:344)\n\tat com.databricks.util.TracingSpanUtils$.$anonfun$withTracing$4(TracingSpanUtils.scala:235)\n\tat com.databricks.util.TracingSpanUtils$.withTracing(TracingSpanUtils.scala:129)\n\tat com.databricks.util.TracingSpanUtils$.withTracing(TracingSpanUtils.scala:233)\n\tat com.databricks.tracing.TracingUtils$.withTracing(TracingUtils.scala:296)\n\tat com.databricks.spark.util.DatabricksTracingHelper.withSpan(DatabricksSparkTracingHelper.scala:112)\n\tat com.databricks.spark.util.DBRTracing$.withSpan(DBRTracing.scala:47)\n\tat org.apache.spark.sql.execution.SQLExecution$.withExecutionPhase(SQLExecution.scala:142)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$7(QueryExecution.scala:873)\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:1524)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$5(QueryExecution.scala:866)\n\tat com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$4(QueryExecution.scala:863)\n\tat com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$3(QueryExecution.scala:863)\n\tat com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:862)\n\tat com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)\n\tat org.apache.spark.sql.execution.QueryExecution.withQueryExecutionId(QueryExecution.scala:850)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:861)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:860)\n\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:860)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$2(QueryExecution.scala:338)\n\tat com.databricks.sql.util.MemoryTrackerHelper.withMemoryTracking(MemoryTrackerHelper.scala:111)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$1(QueryExecution.scala:337)\n\tat scala.util.Try$.apply(Try.scala:217)\n\tat org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1684)\n\tat org.apache.spark.util.LazyTry.tryT$lzycompute(LazyTry.scala:60)\n\tat org.apache.spark.util.LazyTry.tryT(LazyTry.scala:59)\n\tat org.apache.spark.util.LazyTry.get(LazyTry.scala:75)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:398)\n\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:311)\n\tat org.apache.spark.sql.classic.Dataset$.$anonfun$ofRows$1(Dataset.scala:128)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:860)\n\tat org.apache.spark.sql.classic.SparkSession.$anonfun$withActiveAndFrameProfiler$1(SparkSession.scala:1116)\n\tat com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)\n\tat com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:200)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)\n\tat org.apache.spark.sql.classic.SparkSession.withActiveAndFrameProfiler(SparkSession.scala:1116)\n\tat org.apache.spark.sql.classic.Dataset$.ofRows(Dataset.scala:126)\n\tat org.apache.spark.sql.classic.DataFrameReader.load(DataFrameReader.scala:170)\n\tat org.apache.spark.sql.classic.DataFrameReader.load(DataFrameReader.scala:148)\n\tat org.apache.spark.sql.connect.planner.SparkConnectPlanner.transformReadRel(SparkConnectPlanner.scala:2173)\n\tat org.apache.spark.sql.connect.planner.SparkConnectPlanner.$anonfun$transformRelation$1(SparkConnectPlanner.scala:216)\n\tat org.apache.spark.sql.connect.service.SessionHolder.$anonfun$usePlanCache$8(SessionHolder.scala:739)\n\tat org.apache.spark.sql.connect.service.SessionHolder.measureSubtreeRelationNodes(SessionHolder.scala:755)\n\tat org.apache.spark.sql.connect.service.SessionHolder.$anonfun$usePlanCache$6(SessionHolder.scala:738)\n\tat org.apache.spark.util.Utils$.getTryWithCallerStacktrace(Utils.scala:1745)\n\tat org.apache.spark.util.LazyTry.get(LazyTry.scala:75)\n\tat org.apache.spark.sql.execution.QueryExecution.executedPlan(QueryExecution.scala:785)\n\tat com.databricks.spark.sqlgateway.history.SqlExecutionMetrics.$anonfun$setQueryExecution$1(SqlExecutionMetrics.scala:190)\n\tat scala.Option.flatMap(Option.scala:283)\n\tat com.databricks.spark.sqlgateway.history.SqlExecutionMetrics.setQueryExecution(SqlExecutionMetrics.scala:190)\n\tat com.databricks.spark.sqlgateway.history.SqlGatewayHistorySparkListener.$anonfun$onSqlStart$1(SqlGatewayHistorySparkListener.scala:841)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n\tat com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)\n\tat com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:200)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)\n\tat com.databricks.spark.sqlgateway.history.SqlGatewayHistorySparkListener.com$databricks$spark$sqlgateway$history$SqlGatewayHistorySparkListener$$onSqlStart(SqlGatewayHistorySparkListener.scala:789)\n\tat com.databricks.spark.sqlgateway.history.SqlGatewayHistorySparkListener$$anonfun$onOtherEventDefault$1.applyOrElse(SqlGatewayHistorySparkListener.scala:230)\n\tat com.databricks.spark.sqlgateway.history.SqlGatewayHistorySparkListener$$anonfun$onOtherEventDefault$1.applyOrElse(SqlGatewayHistorySparkListener.scala:218)\n\tat scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:35)\n\tat com.databricks.spark.sqlgateway.history.utils.ScriptStatementHelper$$anonfun$onOtherEvent$1.applyOrElse(ScriptStatementHelper.scala:28)\n\tat com.databricks.spark.sqlgateway.history.utils.ScriptStatementHelper$$anonfun$onOtherEvent$1.applyOrElse(ScriptStatementHelper.scala:28)\n\tat scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:270)\n\tat com.databricks.spark.sqlgateway.history.SqlGatewayHistorySparkListener.$anonfun$onOtherEvent$1(SqlGatewayHistorySparkListener.scala:196)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n\tat com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)\n\tat com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:200)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)\n\tat com.databricks.spark.sqlgateway.history.SqlGatewayHistorySparkListener.onOtherEvent(SqlGatewayHistorySparkListener.scala:196)\n\tat org.apache.spark.scheduler.SparkListenerBus.doPostEvent(SparkListenerBus.scala:108)\n\tat org.apache.spark.scheduler.SparkListenerBus.doPostEvent$(SparkListenerBus.scala:28)\n\tat org.apache.spark.scheduler.AsyncEventQueue.doPostEvent(AsyncEventQueue.scala:46)\n\tat org.apache.spark.scheduler.AsyncEventQueue.doPostEvent(AsyncEventQueue.scala:46)\n\tat org.apache.spark.util.ListenerBus.postToAll(ListenerBus.scala:208)\n\tat org.apache.spark.util.ListenerBus.postToAll$(ListenerBus.scala:172)\n\tat org.apache.spark.scheduler.AsyncEventQueue.super$postToAll(AsyncEventQueue.scala:150)\n\tat org.apache.spark.scheduler.AsyncEventQueue.$anonfun$dispatch$1(AsyncEventQueue.scala:150)\n\tat scala.runtime.java8.JFunction0$mcJ$sp.apply(JFunction0$mcJ$sp.scala:17)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)\n\tat org.apache.spark.scheduler.AsyncEventQueue.org$apache$spark$scheduler$AsyncEventQueue$$dispatch(AsyncEventQueue.scala:119)\n\tat org.apache.spark.scheduler.AsyncEventQueue$$anon$2.$anonfun$run$1(AsyncEventQueue.scala:115)\n\tat org.apache.spark.util.Utils$.tryOrStopSparkContext(Utils.scala:1572)\n\tat org.apache.spark.scheduler.AsyncEventQueue$$anon$2.run(AsyncEventQueue.scala:115)",
        "startIndex": null,
        "stopIndex": null
       },
       "stackFrames": [
        "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
        "\u001B[0;31mAnalysisException\u001B[0m                         Traceback (most recent call last)",
        "File \u001B[0;32m<command-5195277928817664>, line 18\u001B[0m\n\u001B[1;32m     16\u001B[0m table_path \u001B[38;5;241m=\u001B[39m BASE_PATH \u001B[38;5;241m+\u001B[39m table_name\n\u001B[1;32m     17\u001B[0m df \u001B[38;5;241m=\u001B[39m spark\u001B[38;5;241m.\u001B[39mread\u001B[38;5;241m.\u001B[39mformat(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mdelta\u001B[39m\u001B[38;5;124m\"\u001B[39m)\u001B[38;5;241m.\u001B[39mload(table_path)\n\u001B[0;32m---> 18\u001B[0m row_count \u001B[38;5;241m=\u001B[39m df\u001B[38;5;241m.\u001B[39mcount()\n\u001B[1;32m     19\u001B[0m size_bytes \u001B[38;5;241m=\u001B[39m \u001B[38;5;28msum\u001B[39m([f\u001B[38;5;241m.\u001B[39msize \u001B[38;5;28;01mfor\u001B[39;00m f \u001B[38;5;129;01min\u001B[39;00m dbutils\u001B[38;5;241m.\u001B[39mfs\u001B[38;5;241m.\u001B[39mls(table_path) \u001B[38;5;28;01mif\u001B[39;00m f\u001B[38;5;241m.\u001B[39mname\u001B[38;5;241m.\u001B[39mendswith(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m.parquet\u001B[39m\u001B[38;5;124m'\u001B[39m)])\n\u001B[1;32m     20\u001B[0m size_mb \u001B[38;5;241m=\u001B[39m size_bytes \u001B[38;5;241m/\u001B[39m (\u001B[38;5;241m1024\u001B[39m \u001B[38;5;241m*\u001B[39m \u001B[38;5;241m1024\u001B[39m)\n",
        "File \u001B[0;32m/databricks/python/lib/python3.12/site-packages/pyspark/sql/connect/dataframe.py:317\u001B[0m, in \u001B[0;36mDataFrame.count\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    314\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mcount\u001B[39m(\u001B[38;5;28mself\u001B[39m) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m \u001B[38;5;28mint\u001B[39m:\n\u001B[1;32m    315\u001B[0m     table, _ \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39magg(\n\u001B[1;32m    316\u001B[0m         F\u001B[38;5;241m.\u001B[39m_invoke_function(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcount\u001B[39m\u001B[38;5;124m\"\u001B[39m, F\u001B[38;5;241m.\u001B[39mlit(\u001B[38;5;241m1\u001B[39m))\n\u001B[0;32m--> 317\u001B[0m     )\u001B[38;5;241m.\u001B[39m_to_table()  \u001B[38;5;66;03m# type: ignore[operator]\u001B[39;00m\n\u001B[1;32m    318\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m table[\u001B[38;5;241m0\u001B[39m][\u001B[38;5;241m0\u001B[39m]\u001B[38;5;241m.\u001B[39mas_py()\n",
        "File \u001B[0;32m/databricks/python/lib/python3.12/site-packages/pyspark/sql/connect/dataframe.py:1890\u001B[0m, in \u001B[0;36mDataFrame._to_table\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m   1888\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_to_table\u001B[39m(\u001B[38;5;28mself\u001B[39m) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Tuple[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mpa.Table\u001B[39m\u001B[38;5;124m\"\u001B[39m, Optional[StructType]]:\n\u001B[1;32m   1889\u001B[0m     query \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_plan\u001B[38;5;241m.\u001B[39mto_proto(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_session\u001B[38;5;241m.\u001B[39mclient)\n\u001B[0;32m-> 1890\u001B[0m     table, schema, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_execution_info \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_session\u001B[38;5;241m.\u001B[39mclient\u001B[38;5;241m.\u001B[39mto_table(\n\u001B[1;32m   1891\u001B[0m         query, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_plan\u001B[38;5;241m.\u001B[39mobservations\n\u001B[1;32m   1892\u001B[0m     )\n\u001B[1;32m   1893\u001B[0m     \u001B[38;5;28;01massert\u001B[39;00m table \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m   1894\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m (table, schema)\n",
        "File \u001B[0;32m/databricks/python/lib/python3.12/site-packages/pyspark/sql/connect/client/core.py:1239\u001B[0m, in \u001B[0;36mSparkConnectClient.to_table\u001B[0;34m(self, plan, observations)\u001B[0m\n\u001B[1;32m   1237\u001B[0m req \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_execute_plan_request_with_metadata()\n\u001B[1;32m   1238\u001B[0m req\u001B[38;5;241m.\u001B[39mplan\u001B[38;5;241m.\u001B[39mCopyFrom(plan)\n\u001B[0;32m-> 1239\u001B[0m table, schema, metrics, observed_metrics, _ \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_execute_and_fetch(req, observations)\n\u001B[1;32m   1241\u001B[0m \u001B[38;5;66;03m# Create a query execution object.\u001B[39;00m\n\u001B[1;32m   1242\u001B[0m ei \u001B[38;5;241m=\u001B[39m ExecutionInfo(metrics, observed_metrics)\n",
        "File \u001B[0;32m/databricks/python/lib/python3.12/site-packages/pyspark/sql/connect/client/core.py:2059\u001B[0m, in \u001B[0;36mSparkConnectClient._execute_and_fetch\u001B[0;34m(self, req, observations, extra_request_metadata, self_destruct)\u001B[0m\n\u001B[1;32m   2056\u001B[0m properties: Dict[\u001B[38;5;28mstr\u001B[39m, Any] \u001B[38;5;241m=\u001B[39m {}\n\u001B[1;32m   2058\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m Progress(handlers\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_progress_handlers, operation_id\u001B[38;5;241m=\u001B[39mreq\u001B[38;5;241m.\u001B[39moperation_id) \u001B[38;5;28;01mas\u001B[39;00m progress:\n\u001B[0;32m-> 2059\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m response \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_execute_and_fetch_as_iterator(\n\u001B[1;32m   2060\u001B[0m         req, observations, extra_request_metadata \u001B[38;5;129;01mor\u001B[39;00m [], progress\u001B[38;5;241m=\u001B[39mprogress\n\u001B[1;32m   2061\u001B[0m     ):\n\u001B[1;32m   2062\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(response, StructType):\n\u001B[1;32m   2063\u001B[0m             schema \u001B[38;5;241m=\u001B[39m response\n",
        "File \u001B[0;32m/databricks/python/lib/python3.12/site-packages/pyspark/sql/connect/client/core.py:2035\u001B[0m, in \u001B[0;36mSparkConnectClient._execute_and_fetch_as_iterator\u001B[0;34m(self, req, observations, extra_request_metadata, progress)\u001B[0m\n\u001B[1;32m   2033\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m kb\n\u001B[1;32m   2034\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m error:\n\u001B[0;32m-> 2035\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_handle_error(error)\n",
        "File \u001B[0;32m/databricks/python/lib/python3.12/site-packages/pyspark/sql/connect/client/core.py:2355\u001B[0m, in \u001B[0;36mSparkConnectClient._handle_error\u001B[0;34m(self, error)\u001B[0m\n\u001B[1;32m   2353\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mthread_local\u001B[38;5;241m.\u001B[39minside_error_handling \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[1;32m   2354\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(error, grpc\u001B[38;5;241m.\u001B[39mRpcError):\n\u001B[0;32m-> 2355\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_handle_rpc_error(error)\n\u001B[1;32m   2356\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m error\n\u001B[1;32m   2357\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n",
        "File \u001B[0;32m/databricks/python/lib/python3.12/site-packages/pyspark/sql/connect/client/core.py:2433\u001B[0m, in \u001B[0;36mSparkConnectClient._handle_rpc_error\u001B[0;34m(self, rpc_error)\u001B[0m\n\u001B[1;32m   2429\u001B[0m             logger\u001B[38;5;241m.\u001B[39mdebug(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mReceived ErrorInfo: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00minfo\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m   2431\u001B[0m             \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_handle_rpc_error_with_error_info(info, status\u001B[38;5;241m.\u001B[39mmessage, status_code)  \u001B[38;5;66;03m# EDGE\u001B[39;00m\n\u001B[0;32m-> 2433\u001B[0m             \u001B[38;5;28;01mraise\u001B[39;00m convert_exception(\n\u001B[1;32m   2434\u001B[0m                 info,\n\u001B[1;32m   2435\u001B[0m                 status\u001B[38;5;241m.\u001B[39mmessage,\n\u001B[1;32m   2436\u001B[0m                 \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_fetch_enriched_error(info),\n\u001B[1;32m   2437\u001B[0m                 \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_display_server_stack_trace(),\n\u001B[1;32m   2438\u001B[0m                 status_code,\n\u001B[1;32m   2439\u001B[0m             ) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m   2441\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m SparkConnectGrpcException(\n\u001B[1;32m   2442\u001B[0m         message\u001B[38;5;241m=\u001B[39mstatus\u001B[38;5;241m.\u001B[39mmessage,\n\u001B[1;32m   2443\u001B[0m         sql_state\u001B[38;5;241m=\u001B[39mErrorCode\u001B[38;5;241m.\u001B[39mCLIENT_UNEXPECTED_MISSING_SQL_STATE,  \u001B[38;5;66;03m# EDGE\u001B[39;00m\n\u001B[1;32m   2444\u001B[0m         grpc_status_code\u001B[38;5;241m=\u001B[39mstatus_code,\n\u001B[1;32m   2445\u001B[0m     ) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m   2446\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n",
        "\u001B[0;31mAnalysisException\u001B[0m: [PATH_NOT_FOUND] Path does not exist: /Volumes/sales_analysis/data/data/dim_date. SQLSTATE: 42K03\n\nJVM stacktrace:\norg.apache.spark.sql.AnalysisException\n\tat org.apache.spark.sql.errors.QueryCompilationErrors$.dataPathNotExistError(QueryCompilationErrors.scala:2655)\n\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.$anonfun$toBaseRelation$1(DeltaTableV2.scala:596)\n\tat com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)\n\tat com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:200)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)\n\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:532)\n\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:530)\n\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.recordFrameProfile(DeltaTableV2.scala:186)\n\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.toBaseRelation$lzycompute(DeltaTableV2.scala:589)\n\tat com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.toBaseRelation(DeltaTableV2.scala:588)\n\tat com.databricks.sql.transaction.tahoe.sources.DeltaDataSource.$anonfun$createRelation$6(DeltaDataSource.scala:416)\n\tat com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)\n\tat com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:200)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)\n\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:532)\n\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:530)\n\tat com.databricks.sql.transaction.tahoe.sources.DeltaDataSource.recordFrameProfile(DeltaDataSource.scala:101)\n\tat com.databricks.sql.transaction.tahoe.sources.DeltaDataSource.createRelation(DeltaDataSource.scala:379)\n\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:448)\n\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource.org$apache$spark$sql$catalyst$analysis$ResolveDataSource$$loadV1BatchSource(ResolveDataSource.scala:310)\n\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource$$anonfun$apply$1.$anonfun$applyOrElse$5(ResolveDataSource.scala:117)\n\tat scala.Option.getOrElse(Option.scala:201)\n\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource$$anonfun$apply$1.applyOrElse(ResolveDataSource.scala:116)\n\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource$$anonfun$apply$1.applyOrElse(ResolveDataSource.scala:64)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$3(AnalysisHelper.scala:141)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:121)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:141)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:418)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:137)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:133)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:45)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp(AnalysisHelper.scala:114)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp$(AnalysisHelper.scala:113)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUp(LogicalPlan.scala:45)\n\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource.apply(ResolveDataSource.scala:64)\n\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource.apply(ResolveDataSource.scala:62)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$17(RuleExecutor.scala:510)\n\tat org.apache.spark.sql.catalyst.rules.RecoverableRuleExecutionHelper.processRule(RuleExecutor.scala:664)\n\tat org.apache.spark.sql.catalyst.rules.RecoverableRuleExecutionHelper.processRule$(RuleExecutor.scala:648)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.processRule(RuleExecutor.scala:144)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$16(RuleExecutor.scala:510)\n\tat com.databricks.spark.util.MemoryTracker$.withThreadAllocatedBytes(MemoryTracker.scala:51)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.measureRule(QueryPlanningTracker.scala:350)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$15(RuleExecutor.scala:508)\n\tat com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)\n\tat com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:200)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$14(RuleExecutor.scala:507)\n\tat scala.collection.LinearSeqOps.foldLeft(LinearSeq.scala:183)\n\tat scala.collection.LinearSeqOps.foldLeft$(LinearSeq.scala:179)\n\tat scala.collection.immutable.List.foldLeft(List.scala:79)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$13(RuleExecutor.scala:499)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n\tat com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)\n\tat com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:200)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.executeBatch$1(RuleExecutor.scala:473)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$23(RuleExecutor.scala:620)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$23$adapted(RuleExecutor.scala:620)\n\tat scala.collection.immutable.List.foreach(List.scala:334)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:620)\n\tat com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)\n\tat com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:200)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:366)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.super$execute(Analyzer.scala:710)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeSameContext$1(Analyzer.scala:710)\n\tat com.databricks.sql.unity.SAMSnapshotHelper$.visitPlansDuringAnalysis(SAMSnapshotHelper.scala:41)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeSameContext(Analyzer.scala:709)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:682)\n\tat org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:453)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:682)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:605)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:354)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:265)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:354)\n\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.resolveInFixedPoint(HybridAnalyzer.scala:437)\n\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.$anonfun$apply$1(HybridAnalyzer.scala:98)\n\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.withTrackedAnalyzerBridgeState(HybridAnalyzer.scala:135)\n\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.apply(HybridAnalyzer.scala:91)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$2(Analyzer.scala:662)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:425)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:662)\n\tat com.databricks.sql.unity.SAMSnapshotHelper$.visitPlansDuringAnalysis(SAMSnapshotHelper.scala:41)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:654)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$3(QueryExecution.scala:356)\n\tat com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)\n\tat com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:200)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:698)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$8(QueryExecution.scala:873)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withExecutionPhase$1(SQLExecution.scala:161)\n\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:348)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)\n\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:344)\n\tat com.databricks.util.TracingSpanUtils$.$anonfun$withTracing$4(TracingSpanUtils.scala:235)\n\tat com.databricks.util.TracingSpanUtils$.withTracing(TracingSpanUtils.scala:129)\n\tat com.databricks.util.TracingSpanUtils$.withTracing(TracingSpanUtils.scala:233)\n\tat com.databricks.tracing.TracingUtils$.withTracing(TracingUtils.scala:296)\n\tat com.databricks.spark.util.DatabricksTracingHelper.withSpan(DatabricksSparkTracingHelper.scala:112)\n\tat com.databricks.spark.util.DBRTracing$.withSpan(DBRTracing.scala:47)\n\tat org.apache.spark.sql.execution.SQLExecution$.withExecutionPhase(SQLExecution.scala:142)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$7(QueryExecution.scala:873)\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:1524)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$5(QueryExecution.scala:866)\n\tat com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$4(QueryExecution.scala:863)\n\tat com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$3(QueryExecution.scala:863)\n\tat com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:862)\n\tat com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)\n\tat org.apache.spark.sql.execution.QueryExecution.withQueryExecutionId(QueryExecution.scala:850)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:861)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:860)\n\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:860)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$2(QueryExecution.scala:338)\n\tat com.databricks.sql.util.MemoryTrackerHelper.withMemoryTracking(MemoryTrackerHelper.scala:111)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$1(QueryExecution.scala:337)\n\tat scala.util.Try$.apply(Try.scala:217)\n\tat org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1684)\n\tat org.apache.spark.util.LazyTry.tryT$lzycompute(LazyTry.scala:60)\n\tat org.apache.spark.util.LazyTry.tryT(LazyTry.scala:59)\n\tat org.apache.spark.util.LazyTry.get(LazyTry.scala:75)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:398)\n\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:311)\n\tat org.apache.spark.sql.classic.Dataset$.$anonfun$ofRows$1(Dataset.scala:128)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:860)\n\tat org.apache.spark.sql.classic.SparkSession.$anonfun$withActiveAndFrameProfiler$1(SparkSession.scala:1116)\n\tat com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)\n\tat com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:200)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)\n\tat org.apache.spark.sql.classic.SparkSession.withActiveAndFrameProfiler(SparkSession.scala:1116)\n\tat org.apache.spark.sql.classic.Dataset$.ofRows(Dataset.scala:126)\n\tat org.apache.spark.sql.classic.DataFrameReader.load(DataFrameReader.scala:170)\n\tat org.apache.spark.sql.classic.DataFrameReader.load(DataFrameReader.scala:148)\n\tat org.apache.spark.sql.connect.planner.SparkConnectPlanner.transformReadRel(SparkConnectPlanner.scala:2173)\n\tat org.apache.spark.sql.connect.planner.SparkConnectPlanner.$anonfun$transformRelation$1(SparkConnectPlanner.scala:216)\n\tat org.apache.spark.sql.connect.service.SessionHolder.$anonfun$usePlanCache$8(SessionHolder.scala:739)\n\tat org.apache.spark.sql.connect.service.SessionHolder.measureSubtreeRelationNodes(SessionHolder.scala:755)\n\tat org.apache.spark.sql.connect.service.SessionHolder.$anonfun$usePlanCache$6(SessionHolder.scala:738)\n\tat org.apache.spark.util.Utils$.getTryWithCallerStacktrace(Utils.scala:1745)\n\tat org.apache.spark.util.LazyTry.get(LazyTry.scala:75)\n\tat org.apache.spark.sql.execution.QueryExecution.executedPlan(QueryExecution.scala:785)\n\tat com.databricks.spark.sqlgateway.history.SqlExecutionMetrics.$anonfun$setQueryExecution$1(SqlExecutionMetrics.scala:190)\n\tat scala.Option.flatMap(Option.scala:283)\n\tat com.databricks.spark.sqlgateway.history.SqlExecutionMetrics.setQueryExecution(SqlExecutionMetrics.scala:190)\n\tat com.databricks.spark.sqlgateway.history.SqlGatewayHistorySparkListener.$anonfun$onSqlStart$1(SqlGatewayHistorySparkListener.scala:841)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n\tat com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)\n\tat com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:200)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)\n\tat com.databricks.spark.sqlgateway.history.SqlGatewayHistorySparkListener.com$databricks$spark$sqlgateway$history$SqlGatewayHistorySparkListener$$onSqlStart(SqlGatewayHistorySparkListener.scala:789)\n\tat com.databricks.spark.sqlgateway.history.SqlGatewayHistorySparkListener$$anonfun$onOtherEventDefault$1.applyOrElse(SqlGatewayHistorySparkListener.scala:230)\n\tat com.databricks.spark.sqlgateway.history.SqlGatewayHistorySparkListener$$anonfun$onOtherEventDefault$1.applyOrElse(SqlGatewayHistorySparkListener.scala:218)\n\tat scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:35)\n\tat com.databricks.spark.sqlgateway.history.utils.ScriptStatementHelper$$anonfun$onOtherEvent$1.applyOrElse(ScriptStatementHelper.scala:28)\n\tat com.databricks.spark.sqlgateway.history.utils.ScriptStatementHelper$$anonfun$onOtherEvent$1.applyOrElse(ScriptStatementHelper.scala:28)\n\tat scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:270)\n\tat com.databricks.spark.sqlgateway.history.SqlGatewayHistorySparkListener.$anonfun$onOtherEvent$1(SqlGatewayHistorySparkListener.scala:196)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n\tat com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)\n\tat com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:200)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)\n\tat com.databricks.spark.sqlgateway.history.SqlGatewayHistorySparkListener.onOtherEvent(SqlGatewayHistorySparkListener.scala:196)\n\tat org.apache.spark.scheduler.SparkListenerBus.doPostEvent(SparkListenerBus.scala:108)\n\tat org.apache.spark.scheduler.SparkListenerBus.doPostEvent$(SparkListenerBus.scala:28)\n\tat org.apache.spark.scheduler.AsyncEventQueue.doPostEvent(AsyncEventQueue.scala:46)\n\tat org.apache.spark.scheduler.AsyncEventQueue.doPostEvent(AsyncEventQueue.scala:46)\n\tat org.apache.spark.util.ListenerBus.postToAll(ListenerBus.scala:208)\n\tat org.apache.spark.util.ListenerBus.postToAll$(ListenerBus.scala:172)\n\tat org.apache.spark.scheduler.AsyncEventQueue.super$postToAll(AsyncEventQueue.scala:150)\n\tat org.apache.spark.scheduler.AsyncEventQueue.$anonfun$dispatch$1(AsyncEventQueue.scala:150)\n\tat scala.runtime.java8.JFunction0$mcJ$sp.apply(JFunction0$mcJ$sp.scala:17)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)\n\tat org.apache.spark.scheduler.AsyncEventQueue.org$apache$spark$scheduler$AsyncEventQueue$$dispatch(AsyncEventQueue.scala:119)\n\tat org.apache.spark.scheduler.AsyncEventQueue$$anon$2.$anonfun$run$1(AsyncEventQueue.scala:115)\n\tat org.apache.spark.util.Utils$.tryOrStopSparkContext(Utils.scala:1572)\n\tat org.apache.spark.scheduler.AsyncEventQueue$$anon$2.run(AsyncEventQueue.scala:115)"
       ],
       "type": "baseError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"\uD83D\uDD0D VERIFYING SAVED DELTA TABLES\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# List all saved tables\n",
    "print(\"\\n\uD83D\uDCC2 Tables in Delta storage:\")\n",
    "tables = dbutils.fs.ls(BASE_PATH)\n",
    "for table in tables:\n",
    "    print(f\"  ✅ {table.name}\")\n",
    "\n",
    "# Check table details\n",
    "print(\"\\n\uD83D\uDCCA Table Details:\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "for table_name in [\"dim_date\", \"dim_customer\", \"dim_product\", \"dim_seller\", \"fact_sales\"]:\n",
    "    table_path = BASE_PATH + table_name\n",
    "    df = spark.read.format(\"delta\").load(table_path)\n",
    "    row_count = df.count()\n",
    "    size_bytes = sum([f.size for f in dbutils.fs.ls(table_path) if f.name.endswith('.parquet')])\n",
    "    size_mb = size_bytes / (1024 * 1024)\n",
    "    \n",
    "    print(f\"  \uD83D\uDCCA {table_name.upper()}\")\n",
    "    print(f\"      Rows: {row_count:,}\")\n",
    "    print(f\"      Size: {size_mb:.2f} MB\")\n",
    "    print()\n",
    "\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6643ffcd-1b69-4795-9dec-7a66451f3199",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 5195277928817660,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "E-Commerce Analytics - Data Exploration",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}